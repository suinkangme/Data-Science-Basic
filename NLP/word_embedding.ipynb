{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW_hOqEbjKUN"
      },
      "source": [
        "### Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFgbh2HxboYk"
      },
      "source": [
        "Consider the limited vocabulary below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtYsABmQi2J4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02978672-3f72-4e45-ea48-168201d045e9"
      },
      "source": [
        "vocab = [\"the\", \"quick\", \"brown\", \"sly\", \"fox\", \"jumped\", \"over\", \"a\", \"lazy\", \"dog\", \"and\",\"found\",\"lion\"]\n",
        "print(len(vocab))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbs1wHxLi5vu"
      },
      "source": [
        "Write a function to create one hot encodings of the words which maps each word to a vector where it's location in the vocab list is 1 and all other entries are zero. For example \"quick\" should map to a torch tensor of dimension 1 with entries 0,1,0.... Create an extra category for words not in the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrLRb9uki6j8"
      },
      "source": [
        "import torch\n",
        "\n",
        "def one_hot_embedding(token, vocab):\n",
        "  \"\"\"\n",
        "  Token should be a list of words or an indvidual word of length W.\n",
        "  The output shouild be a torch tensor fo size W x (V+1) which gives the one hot encoding for all W tokens\n",
        "  \"\"\"\n",
        "  vocab_size = len(vocab)\n",
        "  one_hot_matrix = torch.zeros(len(token), vocab_size + 1)\n",
        "\n",
        "  for i, word in enumerate(token):\n",
        "      if word in vocab:\n",
        "          one_hot_matrix[i, vocab.index(word)] = 1\n",
        "\n",
        "      else:\n",
        "          # extra category for words not in the vocabulary\n",
        "          one_hot_matrix[i, -1] = 1\n",
        "\n",
        "  return one_hot_matrix"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3UfMuPVb_7V"
      },
      "source": [
        "Create a nn.module that takes in a single sentence, encodes the words as embeddings and averages them. The input should be a python list and the output a torch vector of size $D$. For each word you will encode it as a $D$ dimensional vector and average the final embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v__GZavxSD3f"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MyWordEmbeddingBag(nn.Module):\n",
        "    def __init__(self, vocab, dim):\n",
        "        super(MyWordEmbeddingBag, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(len(vocab) + 1, dim, sparse=True)\n",
        "\n",
        "    def forward(self, input_list):\n",
        "        indices = torch.argmax(torch.tensor(one_hot_embedding(input_list, vocab)), dim=1)\n",
        "        offsets = torch.arange(0, len(input_list) + 1, dtype=torch.long)\n",
        "        output = self.embedding(indices, offsets)\n",
        "\n",
        "        avg_embedding = output.mean(dim=0)\n",
        "\n",
        "        return avg_embedding\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMRquhRochR6"
      },
      "source": [
        "Instantiate the model with vectors of size 100 and forward pass the following sentences through your module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm5n_BAVcXsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3179f79-ee79-4e07-8fb0-2062b8c610dc"
      },
      "source": [
        "sent1 = [\"the\", \"quick\", \"brown\"]\n",
        "sent2 = [\"the\", \"sly\", \"fox\", \"jumped\"]\n",
        "sent3 = [\"the\", \"dog\", \"found\",\"a\",\"lion\"]\n",
        "\n",
        "#Instantiate model\n",
        "embedding_dim = 100\n",
        "my_model = MyWordEmbeddingBag(vocab, embedding_dim)\n",
        "\n",
        "#forward pass sentences\n",
        "assert(len(my_model(sent1))==100)\n",
        "assert(len(my_model(sent2))==100)\n",
        "assert(len(my_model(sent3))==100)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-8b328ee6bd8d>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  indices = torch.argmax(torch.tensor(one_hot_embedding(input_list, vocab)), dim=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q3o3S3BSYrG"
      },
      "source": [
        "Compute the euclidean distance between \"fox\" and \"dog\" using the randomly initialized embedding table in your model above. Note as this is randomly initialized the distances will also be random in this case, however a trained model using word embeddings will often exhibit closer distances between related words, depending on objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mVfbx9Qm2-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4035473-ff55-4de7-a6ae-d26644c6867b"
      },
      "source": [
        "# Compute the euclidean distance between \"fox\" and \"dog\"\n",
        "fox = vocab.index(\"fox\")\n",
        "dog  = vocab.index(\"dog\")\n",
        "\n",
        "# extract the weights from the model\n",
        "embedding_weights = my_model.embedding.weight.detach()\n",
        "\n",
        "# get the embeddings for \"fox\" and \"dog\"\n",
        "fox_embedding = embedding_weights[fox]\n",
        "dog_embedding = embedding_weights[dog]\n",
        "\n",
        "# compute Euclidean distance\n",
        "euclidean_distance = torch.dist(fox_embedding, dog_embedding)\n",
        "\n",
        "print(f\"Euclidean distance between 'fox' and 'dog': {euclidean_distance.item():.2f}\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean distance between 'fox' and 'dog': 14.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01PBNGFzi-yt"
      },
      "source": [
        "### Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR0EGjcKizzi"
      },
      "source": [
        "We will experiment with recurrent networks using the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axkzlu9BlNVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb31a053-1a87-444b-fbc1-68a4164613a9"
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "### Hotfix for very recent MNIST download issue https://github.com/pytorch/vision/issues/1938\n",
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "\n",
        "dataset = torchvision.datasets.MNIST('./', download=True, transform=transforms.Compose([transforms.ToTensor()]), train=True)\n",
        "train_indices = torch.arange(0, 10000)\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "\n",
        "dataset=torchvision.datasets.MNIST('./', download=True, transform=transforms.Compose([transforms.ToTensor()]), train=False)\n",
        "test_indices = torch.arange(0, 10000)\n",
        "test_dataset = Subset(dataset, test_indices)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 335158255.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 39586828.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 29839458.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6743550.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCnDNfcPoIsl"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,\n",
        "                                          shuffle=False, num_workers=0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZQ0AVHJlNyM"
      },
      "source": [
        "Consider the following script (modified from https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/recurrent_neural_network/main.py) which trains an RNN on the MNIST data. Here we can consider each column of the image as an input for each step of the RNN. After 28 steps the model applies a linear layer + cross-entropy. We will use this to familiarize ourselves with the nn.RNN module and the nn.LSTM module. First run the cell below\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBGhHQv5k6uD",
        "outputId": "f01accbb-6704-4041-88c2-94b0682999d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 2\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "# Recurrent neural network (many-to-one)\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out , _ = self.rnn(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        #Gradient clipping\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
        "\n",
        "        # Print the gradient norm for model.rnn.weight_ih_l0 after the first minibatch\n",
        "        if i == 0:\n",
        "            weight_ih_grad_norm = torch.norm(model.rnn.weight_ih_l0.grad)\n",
        "            print(f'gradient norm of model.rnn.weight_ih_l0 after first minibatch: {weight_ih_grad_norm.item()}')\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradient norm of model.rnn.weight_ih_l0 after first minibatch: 0.04013664647936821\n",
            "Epoch [1/2], Step [10/157], Loss: 2.3971\n",
            "Epoch [1/2], Step [20/157], Loss: 2.4714\n",
            "Epoch [1/2], Step [30/157], Loss: 2.3240\n",
            "Epoch [1/2], Step [40/157], Loss: 2.3662\n",
            "Epoch [1/2], Step [50/157], Loss: 2.3167\n",
            "Epoch [1/2], Step [60/157], Loss: 2.3435\n",
            "Epoch [1/2], Step [70/157], Loss: 2.3859\n",
            "Epoch [1/2], Step [80/157], Loss: 2.4344\n",
            "Epoch [1/2], Step [90/157], Loss: 2.3919\n",
            "Epoch [1/2], Step [100/157], Loss: 2.4995\n",
            "Epoch [1/2], Step [110/157], Loss: 2.4760\n",
            "Epoch [1/2], Step [120/157], Loss: 2.3804\n",
            "Epoch [1/2], Step [130/157], Loss: 2.4192\n",
            "Epoch [1/2], Step [140/157], Loss: 2.3308\n",
            "Epoch [1/2], Step [150/157], Loss: 2.4480\n",
            "gradient norm of model.rnn.weight_ih_l0 after first minibatch: 2.87736675090855e-05\n",
            "Epoch [2/2], Step [10/157], Loss: 2.2943\n",
            "Epoch [2/2], Step [20/157], Loss: 2.3986\n",
            "Epoch [2/2], Step [30/157], Loss: 2.4726\n",
            "Epoch [2/2], Step [40/157], Loss: 2.3181\n",
            "Epoch [2/2], Step [50/157], Loss: 2.3061\n",
            "Epoch [2/2], Step [60/157], Loss: 2.3323\n",
            "Epoch [2/2], Step [70/157], Loss: 2.4207\n",
            "Epoch [2/2], Step [80/157], Loss: 2.3973\n",
            "Epoch [2/2], Step [90/157], Loss: 2.4760\n",
            "Epoch [2/2], Step [100/157], Loss: 2.3248\n",
            "Epoch [2/2], Step [110/157], Loss: 2.2670\n",
            "Epoch [2/2], Step [120/157], Loss: 2.4398\n",
            "Epoch [2/2], Step [130/157], Loss: 2.3731\n",
            "Epoch [2/2], Step [140/157], Loss: 2.3500\n",
            "Epoch [2/2], Step [150/157], Loss: 2.4357\n",
            "Test Accuracy of the model on the 10000 test images: 10.1 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW9D4GAUziV4"
      },
      "source": [
        "Modify the above code (no need to create a new cell) to print the gradient norm of some of the parameters after backward in the the first minibatch. Do this for the following weight parameter: model.rnn.weight_ih_l0. -> implemented in above cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuIgmoPExB5O"
      },
      "source": [
        "Modify the code (in a new cell below) to use LSTM  (and remove the gradient clipping) and rerun the code. Note this is essentially what is done in the original script linked above which you may check for reference or if you get stuck. Run with LSTM and compare the accuracy and the gradient norm for weight_ih_l0 of the recurrent network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPrhmH231cVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c579010b-3443-419c-931d-96c581c838b7"
      },
      "source": [
        "# LSTM\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out\n",
        "\n",
        "model_lstm = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_lstm = torch.optim.Adam(model_lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_lstm(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer_lstm.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Print the gradient norm for model_lstm.lstm.weight_ih_l0 after the first minibatch\n",
        "        if i == 0:\n",
        "            weight_ih_l0_grad_norm = torch.norm(model_lstm.lstm.weight_ih_l0.grad)\n",
        "            print(f'Gradient Norm of model_lstm.lstm.weight_ih_l0 after first minibatch: {weight_ih_l0_grad_norm.item()}')\n",
        "\n",
        "        optimizer_lstm.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "model_lstm.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model_lstm(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the LSTM model on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Norm of model_lstm.lstm.weight_ih_l0 after first minibatch: 0.007739682216197252\n",
            "Epoch [1/2], Step [10/157], Loss: 2.2484\n",
            "Epoch [1/2], Step [20/157], Loss: 1.9376\n",
            "Epoch [1/2], Step [30/157], Loss: 1.2920\n",
            "Epoch [1/2], Step [40/157], Loss: 1.4531\n",
            "Epoch [1/2], Step [50/157], Loss: 0.7231\n",
            "Epoch [1/2], Step [60/157], Loss: 0.9452\n",
            "Epoch [1/2], Step [70/157], Loss: 0.7732\n",
            "Epoch [1/2], Step [80/157], Loss: 0.6432\n",
            "Epoch [1/2], Step [90/157], Loss: 0.8362\n",
            "Epoch [1/2], Step [100/157], Loss: 0.5037\n",
            "Epoch [1/2], Step [110/157], Loss: 0.6440\n",
            "Epoch [1/2], Step [120/157], Loss: 0.4464\n",
            "Epoch [1/2], Step [130/157], Loss: 0.5817\n",
            "Epoch [1/2], Step [140/157], Loss: 0.2926\n",
            "Epoch [1/2], Step [150/157], Loss: 0.5951\n",
            "Gradient Norm of model_lstm.lstm.weight_ih_l0 after first minibatch: 0.3933633863925934\n",
            "Epoch [2/2], Step [10/157], Loss: 0.3511\n",
            "Epoch [2/2], Step [20/157], Loss: 0.3241\n",
            "Epoch [2/2], Step [30/157], Loss: 0.3752\n",
            "Epoch [2/2], Step [40/157], Loss: 0.4754\n",
            "Epoch [2/2], Step [50/157], Loss: 0.3697\n",
            "Epoch [2/2], Step [60/157], Loss: 0.3560\n",
            "Epoch [2/2], Step [70/157], Loss: 0.3302\n",
            "Epoch [2/2], Step [80/157], Loss: 0.4053\n",
            "Epoch [2/2], Step [90/157], Loss: 0.3454\n",
            "Epoch [2/2], Step [100/157], Loss: 0.3018\n",
            "Epoch [2/2], Step [110/157], Loss: 0.1821\n",
            "Epoch [2/2], Step [120/157], Loss: 0.1436\n",
            "Epoch [2/2], Step [130/157], Loss: 0.2891\n",
            "Epoch [2/2], Step [140/157], Loss: 0.1746\n",
            "Epoch [2/2], Step [150/157], Loss: 0.2993\n",
            "Test Accuracy of the LSTM model on the 10000 test images: 93.64 %\n"
          ]
        }
      ]
    }
  ]
}
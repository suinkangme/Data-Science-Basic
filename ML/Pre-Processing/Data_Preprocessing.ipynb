{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Value: [0 3 4 1 2 2 5 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "items=['TV','frige','microwave','computer','fan','fan','mixer','mixer']\n",
    "\n",
    "# Create LabelEncoder object, perfore encoding with fit(), transform().\n",
    "encoder = LabelEncoder()\n",
    "# fit() used a lot in scikit-learn, it could be used when we do fit()&predit() in supervised learning and this case : fit()->transform()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "print('Encoding Value:',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding class: ['TV' 'computer' 'fan' 'frige' 'microwave' 'mixer']\n"
     ]
    }
   ],
   "source": [
    "print('Encoding class:',encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding Original value: ['microwave' 'mixer' 'fan' 'TV' 'computer' 'computer' 'frige' 'frige']\n"
     ]
    }
   ],
   "source": [
    "# inverse_transform : convert number to string value\n",
    "print('Decoding Original value:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoding data\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "one-hot encoding data shape : (8, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "items=['TV','frige','microwave','computer','fan','fan','mixer','mixer']\n",
    "\n",
    "# convert it to two-dimensional ndarray.\n",
    "items = np.array(items).reshape(-1, 1)\n",
    "\n",
    "# apply one-hot encoding : fit_transform()\n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_labels = oh_encoder.fit_transform(items)\n",
    "\n",
    "# the converted result of OneHotEncoder is Sparse Matrix, so it needs to be converted to Dense Matrix by toarray().\n",
    "print('one-hot encoding data')\n",
    "print(oh_labels.toarray())\n",
    "print('one-hot encoding data shape :', oh_labels.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_TV</th>\n",
       "      <th>item_computer</th>\n",
       "      <th>item_fan</th>\n",
       "      <th>item_frige</th>\n",
       "      <th>item_microwave</th>\n",
       "      <th>item_mixer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_TV  item_computer  item_fan  item_frige  item_microwave  item_mixer\n",
       "0        1              0         0           0               0           0\n",
       "1        0              0         0           1               0           0\n",
       "2        0              0         0           0               1           0\n",
       "3        0              1         0           0               0           0\n",
       "4        0              0         1           0               0           0\n",
       "5        0              0         1           0               0           0\n",
       "6        0              0         0           0               0           1\n",
       "7        0              0         0           0               0           1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'item':['TV','frige','microwave','computer','fan','fan','mixer','mixer'] })\n",
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featrue Scaling and standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of feature values\n",
      "sepal length (cm)    5.843333\n",
      "sepal width (cm)     3.057333\n",
      "petal length (cm)    3.758000\n",
      "petal width (cm)     1.199333\n",
      "dtype: float64\n",
      "\n",
      "the variation of feature vaules\n",
      "sepal length (cm)    0.685694\n",
      "sepal width (cm)     0.189979\n",
      "petal length (cm)    3.116278\n",
      "petal width (cm)     0.581006\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "# Load Iris datasets and convert to DataFrame.\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "\n",
    "print('mean of feature values')\n",
    "print(iris_df.mean())\n",
    "print('\\nthe variation of feature vaules')\n",
    "print(iris_df.var())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of feature values\n",
      "sepal length (cm)   -1.690315e-15\n",
      "sepal width (cm)    -1.842970e-15\n",
      "petal length (cm)   -1.698641e-15\n",
      "petal width (cm)    -1.409243e-15\n",
      "dtype: float64\n",
      "\n",
      "the variation of feature vaules\n",
      "sepal length (cm)    1.006711\n",
      "sepal width (cm)     1.006711\n",
      "petal length (cm)    1.006711\n",
      "petal width (cm)     1.006711\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "# convert the datasets to StandardScaler. call fit( ), transform( ).\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "# after transform( ) the dataset's scale changed to numpy ndarry, so it needs to be converted to DataFrame.\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('mean of feature values')\n",
    "print(iris_df_scaled.mean())\n",
    "print('\\nthe variation of feature vaules')\n",
    "print(iris_df_scaled.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min of feature values\n",
      "sepal length (cm)    0.0\n",
      "sepal width (cm)     0.0\n",
      "petal length (cm)    0.0\n",
      "petal width (cm)     0.0\n",
      "dtype: float64\n",
      "\n",
      "max of feature values\n",
      "sepal length (cm)    1.0\n",
      "sepal width (cm)     1.0\n",
      "petal length (cm)    1.0\n",
      "petal width (cm)     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create MinMaxScaler object.\n",
    "scaler = MinMaxScaler()\n",
    "# convert the datasets to MinMaxScaler. call fit(), transform().\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "# after transform( ) the dataset's scale changed to numpy ndarry, so it needs to be converted to DataFrame.\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "print('min of feature values')\n",
    "print(iris_df_scaled.min())\n",
    "print('\\nmax of feature values')\n",
    "print(iris_df_scaled.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NOTE! Use Scaler to training and test dataset applying fit(), transform(), fit_transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# training data set is created with values ranging from 0 to 10, while the test data set is created with values ranging from 0 to 5.\n",
    "# methods fit() and transform() of the Scaler class only accept 2-dimensional data. Therefore, you need to reshape the data using reshape(-1, 1) to change its dimension to a 2-dimensional array.\n",
    "train_array = np.arange(0, 11).reshape(-1, 1)\n",
    "test_array =  np.arange(0, 6).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train_array data: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "train_array data after Scaling : [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "# To create a MinMaxScaler object that transforms the data to have a minimum value of 0 and a maximum value of 1.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# after fit(), min of train_array 0 and max will be 10.  \n",
    "scaler.fit(train_array)\n",
    "\n",
    "# the train_array converted with 1/10 scale, the original will be converted 10-> 1\n",
    "train_scaled = scaler.transform(train_array)\n",
    " \n",
    "print('original train_array data:', np.round(train_array.reshape(-1), 2))\n",
    "print('train_array data after Scaling :', np.round(train_scaled.reshape(-1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original test_array data: [0 1 2 3 4 5]\n",
      "test_array data after scaling: [0.  0.2 0.4 0.6 0.8 1. ]\n"
     ]
    }
   ],
   "source": [
    "# If you call the fit() method on the MinMaxScaler object using the test_array, the minimum and maximum values of the test data will be used to determine the scaling factors. \n",
    "# min of original data :0, max :5\n",
    "scaler.fit(test_array)\n",
    "# test_array converted with 1/5 scale. original will be conveted 5->1.  \n",
    "test_scaled = scaler.transform(test_array)\n",
    "# print converted train_array\n",
    "print('original test_array data:', np.round(test_array.reshape(-1), 2))\n",
    "print('test_array data after scaling:', np.round(test_scaled.reshape(-1), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train_array data: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "train_array data after scaling: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "original test_array data: [0 1 2 3 4 5]\n",
      "test_array data after Scaling: [0.  0.1 0.2 0.3 0.4 0.5]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_array)\n",
    "train_scaled = scaler.transform(train_array)\n",
    "print('original train_array data:', np.round(train_array.reshape(-1), 2))\n",
    "print('train_array data after scaling:', np.round(train_scaled.reshape(-1), 2))\n",
    "\n",
    "# when you want to apply the scaling transformation to the test_array using the MinMaxScaler object, you should not call the fit() method again.\n",
    "# only transform()! \n",
    "test_scaled = scaler.transform(test_array)\n",
    "print('\\noriginal test_array data:', np.round(test_array.reshape(-1), 2))\n",
    "print('test_array data after Scaling:', np.round(test_scaled.reshape(-1), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

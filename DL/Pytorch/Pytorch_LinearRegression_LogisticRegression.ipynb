{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Basic_Deep Learning"
      ],
      "metadata": {
        "id": "OuS04gqWrlBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "plbuWv54rsMP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear *Regression*"
      ],
      "metadata": {
        "id": "MoZzMX2io0jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch import tensor"
      ],
      "metadata": {
        "id": "yKqXSVRTNn8T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])"
      ],
      "metadata": {
        "id": "SRgFv3BVY6WC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "wtfH_qG3P3K8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct our loss function and an Optimizer\n",
        "- The call to model.parameters()\n",
        "\n",
        "In the SGD constructor will contain the learnable parameters of the two nn.linear modules which are members of the model."
      ],
      "metadata": {
        "id": "DRyCsvZFQuqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGELLaQnQklR",
        "outputId": "6e74f034-20ef-442d-ba39-f871824b2183"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 105.5473403930664 \n",
            "Epoch: 1 | Loss: 47.08689498901367 \n",
            "Epoch: 2 | Loss: 21.060535430908203 \n",
            "Epoch: 3 | Loss: 9.472921371459961 \n",
            "Epoch: 4 | Loss: 4.313045501708984 \n",
            "Epoch: 5 | Loss: 2.014634132385254 \n",
            "Epoch: 6 | Loss: 0.9900877475738525 \n",
            "Epoch: 7 | Loss: 0.5326491594314575 \n",
            "Epoch: 8 | Loss: 0.3276897370815277 \n",
            "Epoch: 9 | Loss: 0.23514652252197266 \n",
            "Epoch: 10 | Loss: 0.1926654875278473 \n",
            "Epoch: 11 | Loss: 0.1724899858236313 \n",
            "Epoch: 12 | Loss: 0.162261962890625 \n",
            "Epoch: 13 | Loss: 0.1564803421497345 \n",
            "Epoch: 14 | Loss: 0.15269571542739868 \n",
            "Epoch: 15 | Loss: 0.14981773495674133 \n",
            "Epoch: 16 | Loss: 0.14736014604568481 \n",
            "Epoch: 17 | Loss: 0.14510688185691833 \n",
            "Epoch: 18 | Loss: 0.1429610550403595 \n",
            "Epoch: 19 | Loss: 0.14087967574596405 \n",
            "Epoch: 20 | Loss: 0.1388430893421173 \n",
            "Epoch: 21 | Loss: 0.136842280626297 \n",
            "Epoch: 22 | Loss: 0.13487327098846436 \n",
            "Epoch: 23 | Loss: 0.13293400406837463 \n",
            "Epoch: 24 | Loss: 0.13102290034294128 \n",
            "Epoch: 25 | Loss: 0.12913981080055237 \n",
            "Epoch: 26 | Loss: 0.12728382647037506 \n",
            "Epoch: 27 | Loss: 0.12545451521873474 \n",
            "Epoch: 28 | Loss: 0.12365151941776276 \n",
            "Epoch: 29 | Loss: 0.12187428772449493 \n",
            "Epoch: 30 | Loss: 0.12012286484241486 \n",
            "Epoch: 31 | Loss: 0.11839655041694641 \n",
            "Epoch: 32 | Loss: 0.11669491231441498 \n",
            "Epoch: 33 | Loss: 0.11501786857843399 \n",
            "Epoch: 34 | Loss: 0.11336494982242584 \n",
            "Epoch: 35 | Loss: 0.11173558980226517 \n",
            "Epoch: 36 | Loss: 0.11012974381446838 \n",
            "Epoch: 37 | Loss: 0.10854725539684296 \n",
            "Epoch: 38 | Loss: 0.10698719322681427 \n",
            "Epoch: 39 | Loss: 0.10544948279857635 \n",
            "Epoch: 40 | Loss: 0.10393399745225906 \n",
            "Epoch: 41 | Loss: 0.10244029760360718 \n",
            "Epoch: 42 | Loss: 0.10096804052591324 \n",
            "Epoch: 43 | Loss: 0.09951716661453247 \n",
            "Epoch: 44 | Loss: 0.09808683395385742 \n",
            "Epoch: 45 | Loss: 0.09667709469795227 \n",
            "Epoch: 46 | Loss: 0.09528777003288269 \n",
            "Epoch: 47 | Loss: 0.09391836822032928 \n",
            "Epoch: 48 | Loss: 0.0925685316324234 \n",
            "Epoch: 49 | Loss: 0.09123817086219788 \n",
            "Epoch: 50 | Loss: 0.08992689847946167 \n",
            "Epoch: 51 | Loss: 0.08863452076911926 \n",
            "Epoch: 52 | Loss: 0.08736078441143036 \n",
            "Epoch: 53 | Loss: 0.08610540628433228 \n",
            "Epoch: 54 | Loss: 0.084867924451828 \n",
            "Epoch: 55 | Loss: 0.08364808559417725 \n",
            "Epoch: 56 | Loss: 0.0824458971619606 \n",
            "Epoch: 57 | Loss: 0.08126113563776016 \n",
            "Epoch: 58 | Loss: 0.08009327948093414 \n",
            "Epoch: 59 | Loss: 0.07894206047058105 \n",
            "Epoch: 60 | Loss: 0.07780776917934418 \n",
            "Epoch: 61 | Loss: 0.07668951153755188 \n",
            "Epoch: 62 | Loss: 0.075587198138237 \n",
            "Epoch: 63 | Loss: 0.07450100779533386 \n",
            "Epoch: 64 | Loss: 0.07343025505542755 \n",
            "Epoch: 65 | Loss: 0.07237491756677628 \n",
            "Epoch: 66 | Loss: 0.0713348314166069 \n",
            "Epoch: 67 | Loss: 0.07030965387821198 \n",
            "Epoch: 68 | Loss: 0.06929921358823776 \n",
            "Epoch: 69 | Loss: 0.06830322742462158 \n",
            "Epoch: 70 | Loss: 0.06732157617807388 \n",
            "Epoch: 71 | Loss: 0.06635408103466034 \n",
            "Epoch: 72 | Loss: 0.06540050357580185 \n",
            "Epoch: 73 | Loss: 0.0644606351852417 \n",
            "Epoch: 74 | Loss: 0.06353426724672318 \n",
            "Epoch: 75 | Loss: 0.0626210868358612 \n",
            "Epoch: 76 | Loss: 0.061721108853816986 \n",
            "Epoch: 77 | Loss: 0.06083410978317261 \n",
            "Epoch: 78 | Loss: 0.05995991826057434 \n",
            "Epoch: 79 | Loss: 0.05909813195466995 \n",
            "Epoch: 80 | Loss: 0.05824868381023407 \n",
            "Epoch: 81 | Loss: 0.057411663234233856 \n",
            "Epoch: 82 | Loss: 0.05658656731247902 \n",
            "Epoch: 83 | Loss: 0.05577326565980911 \n",
            "Epoch: 84 | Loss: 0.054971806704998016 \n",
            "Epoch: 85 | Loss: 0.0541817843914032 \n",
            "Epoch: 86 | Loss: 0.05340299382805824 \n",
            "Epoch: 87 | Loss: 0.052635662257671356 \n",
            "Epoch: 88 | Loss: 0.05187899246811867 \n",
            "Epoch: 89 | Loss: 0.0511334203183651 \n",
            "Epoch: 90 | Loss: 0.05039869621396065 \n",
            "Epoch: 91 | Loss: 0.04967431724071503 \n",
            "Epoch: 92 | Loss: 0.04896046966314316 \n",
            "Epoch: 93 | Loss: 0.048256874084472656 \n",
            "Epoch: 94 | Loss: 0.047563351690769196 \n",
            "Epoch: 95 | Loss: 0.046879686415195465 \n",
            "Epoch: 96 | Loss: 0.04620609059929848 \n",
            "Epoch: 97 | Loss: 0.045541901141405106 \n",
            "Epoch: 98 | Loss: 0.044887423515319824 \n",
            "Epoch: 99 | Loss: 0.04424232989549637 \n",
            "Epoch: 100 | Loss: 0.04360652714967728 \n",
            "Epoch: 101 | Loss: 0.04297986254096031 \n",
            "Epoch: 102 | Loss: 0.04236216098070145 \n",
            "Epoch: 103 | Loss: 0.04175340011715889 \n",
            "Epoch: 104 | Loss: 0.04115317761898041 \n",
            "Epoch: 105 | Loss: 0.04056183248758316 \n",
            "Epoch: 106 | Loss: 0.039978861808776855 \n",
            "Epoch: 107 | Loss: 0.03940434753894806 \n",
            "Epoch: 108 | Loss: 0.03883800655603409 \n",
            "Epoch: 109 | Loss: 0.0382799357175827 \n",
            "Epoch: 110 | Loss: 0.03772969916462898 \n",
            "Epoch: 111 | Loss: 0.037187471985816956 \n",
            "Epoch: 112 | Loss: 0.03665308654308319 \n",
            "Epoch: 113 | Loss: 0.03612634539604187 \n",
            "Epoch: 114 | Loss: 0.03560712933540344 \n",
            "Epoch: 115 | Loss: 0.035095322877168655 \n",
            "Epoch: 116 | Loss: 0.03459098935127258 \n",
            "Epoch: 117 | Loss: 0.03409387543797493 \n",
            "Epoch: 118 | Loss: 0.03360392898321152 \n",
            "Epoch: 119 | Loss: 0.03312092274427414 \n",
            "Epoch: 120 | Loss: 0.03264487907290459 \n",
            "Epoch: 121 | Loss: 0.03217577934265137 \n",
            "Epoch: 122 | Loss: 0.03171340748667717 \n",
            "Epoch: 123 | Loss: 0.031257618218660355 \n",
            "Epoch: 124 | Loss: 0.030808396637439728 \n",
            "Epoch: 125 | Loss: 0.0303655955940485 \n",
            "Epoch: 126 | Loss: 0.029929159209132195 \n",
            "Epoch: 127 | Loss: 0.029499119147658348 \n",
            "Epoch: 128 | Loss: 0.029075123369693756 \n",
            "Epoch: 129 | Loss: 0.028657305985689163 \n",
            "Epoch: 130 | Loss: 0.028245504945516586 \n",
            "Epoch: 131 | Loss: 0.027839498594403267 \n",
            "Epoch: 132 | Loss: 0.027439463883638382 \n",
            "Epoch: 133 | Loss: 0.027045106515288353 \n",
            "Epoch: 134 | Loss: 0.02665635570883751 \n",
            "Epoch: 135 | Loss: 0.02627331204712391 \n",
            "Epoch: 136 | Loss: 0.02589566633105278 \n",
            "Epoch: 137 | Loss: 0.02552349865436554 \n",
            "Epoch: 138 | Loss: 0.02515670843422413 \n",
            "Epoch: 139 | Loss: 0.0247951652854681 \n",
            "Epoch: 140 | Loss: 0.024438846856355667 \n",
            "Epoch: 141 | Loss: 0.024087589234113693 \n",
            "Epoch: 142 | Loss: 0.02374142035841942 \n",
            "Epoch: 143 | Loss: 0.023400280624628067 \n",
            "Epoch: 144 | Loss: 0.023063914850354195 \n",
            "Epoch: 145 | Loss: 0.022732403129339218 \n",
            "Epoch: 146 | Loss: 0.022405773401260376 \n",
            "Epoch: 147 | Loss: 0.022083820775151253 \n",
            "Epoch: 148 | Loss: 0.021766403689980507 \n",
            "Epoch: 149 | Loss: 0.021453551948070526 \n",
            "Epoch: 150 | Loss: 0.021145178005099297 \n",
            "Epoch: 151 | Loss: 0.020841294899582863 \n",
            "Epoch: 152 | Loss: 0.020541802048683167 \n",
            "Epoch: 153 | Loss: 0.020246636122465134 \n",
            "Epoch: 154 | Loss: 0.01995561085641384 \n",
            "Epoch: 155 | Loss: 0.019668787717819214 \n",
            "Epoch: 156 | Loss: 0.01938614621758461 \n",
            "Epoch: 157 | Loss: 0.019107535481452942 \n",
            "Epoch: 158 | Loss: 0.018832992762327194 \n",
            "Epoch: 159 | Loss: 0.018562234938144684 \n",
            "Epoch: 160 | Loss: 0.018295519053936005 \n",
            "Epoch: 161 | Loss: 0.018032586202025414 \n",
            "Epoch: 162 | Loss: 0.017773400992155075 \n",
            "Epoch: 163 | Loss: 0.017518015578389168 \n",
            "Epoch: 164 | Loss: 0.01726624183356762 \n",
            "Epoch: 165 | Loss: 0.017018083482980728 \n",
            "Epoch: 166 | Loss: 0.01677354797720909 \n",
            "Epoch: 167 | Loss: 0.016532445326447487 \n",
            "Epoch: 168 | Loss: 0.016294866800308228 \n",
            "Epoch: 169 | Loss: 0.016060709953308105 \n",
            "Epoch: 170 | Loss: 0.015829836949706078 \n",
            "Epoch: 171 | Loss: 0.015602363273501396 \n",
            "Epoch: 172 | Loss: 0.015378165990114212 \n",
            "Epoch: 173 | Loss: 0.015157178975641727 \n",
            "Epoch: 174 | Loss: 0.014939234592020512 \n",
            "Epoch: 175 | Loss: 0.014724625274538994 \n",
            "Epoch: 176 | Loss: 0.014512947760522366 \n",
            "Epoch: 177 | Loss: 0.014304382726550102 \n",
            "Epoch: 178 | Loss: 0.014098824001848698 \n",
            "Epoch: 179 | Loss: 0.013896185904741287 \n",
            "Epoch: 180 | Loss: 0.013696478679776192 \n",
            "Epoch: 181 | Loss: 0.013499640859663486 \n",
            "Epoch: 182 | Loss: 0.013305631466209888 \n",
            "Epoch: 183 | Loss: 0.013114409521222115 \n",
            "Epoch: 184 | Loss: 0.01292591542005539 \n",
            "Epoch: 185 | Loss: 0.012740161269903183 \n",
            "Epoch: 186 | Loss: 0.012557102367281914 \n",
            "Epoch: 187 | Loss: 0.012376641854643822 \n",
            "Epoch: 188 | Loss: 0.012198777869343758 \n",
            "Epoch: 189 | Loss: 0.01202341914176941 \n",
            "Epoch: 190 | Loss: 0.011850621551275253 \n",
            "Epoch: 191 | Loss: 0.011680357158184052 \n",
            "Epoch: 192 | Loss: 0.011512471362948418 \n",
            "Epoch: 193 | Loss: 0.011347009800374508 \n",
            "Epoch: 194 | Loss: 0.011183944530785084 \n",
            "Epoch: 195 | Loss: 0.011023243889212608 \n",
            "Epoch: 196 | Loss: 0.010864771902561188 \n",
            "Epoch: 197 | Loss: 0.010708652436733246 \n",
            "Epoch: 198 | Loss: 0.010554706677794456 \n",
            "Epoch: 199 | Loss: 0.010403050109744072 \n",
            "Epoch: 200 | Loss: 0.010253528133034706 \n",
            "Epoch: 201 | Loss: 0.010106226429343224 \n",
            "Epoch: 202 | Loss: 0.009960914961993694 \n",
            "Epoch: 203 | Loss: 0.009817754849791527 \n",
            "Epoch: 204 | Loss: 0.009676706977188587 \n",
            "Epoch: 205 | Loss: 0.009537622332572937 \n",
            "Epoch: 206 | Loss: 0.009400558657944202 \n",
            "Epoch: 207 | Loss: 0.009265477769076824 \n",
            "Epoch: 208 | Loss: 0.00913228653371334 \n",
            "Epoch: 209 | Loss: 0.009001052007079124 \n",
            "Epoch: 210 | Loss: 0.008871681056916714 \n",
            "Epoch: 211 | Loss: 0.008744128048419952 \n",
            "Epoch: 212 | Loss: 0.008618498221039772 \n",
            "Epoch: 213 | Loss: 0.008494628593325615 \n",
            "Epoch: 214 | Loss: 0.008372537791728973 \n",
            "Epoch: 215 | Loss: 0.008252224884927273 \n",
            "Epoch: 216 | Loss: 0.008133605122566223 \n",
            "Epoch: 217 | Loss: 0.008016745559871197 \n",
            "Epoch: 218 | Loss: 0.007901547476649284 \n",
            "Epoch: 219 | Loss: 0.007787983864545822 \n",
            "Epoch: 220 | Loss: 0.007676031906157732 \n",
            "Epoch: 221 | Loss: 0.007565729785710573 \n",
            "Epoch: 222 | Loss: 0.007457014638930559 \n",
            "Epoch: 223 | Loss: 0.007349833846092224 \n",
            "Epoch: 224 | Loss: 0.007244210224598646 \n",
            "Epoch: 225 | Loss: 0.007140073925256729 \n",
            "Epoch: 226 | Loss: 0.007037500850856304 \n",
            "Epoch: 227 | Loss: 0.006936309393495321 \n",
            "Epoch: 228 | Loss: 0.006836648564785719 \n",
            "Epoch: 229 | Loss: 0.006738387048244476 \n",
            "Epoch: 230 | Loss: 0.006641589570790529 \n",
            "Epoch: 231 | Loss: 0.0065461136400699615 \n",
            "Epoch: 232 | Loss: 0.006452067289501429 \n",
            "Epoch: 233 | Loss: 0.00635931733995676 \n",
            "Epoch: 234 | Loss: 0.006267893128097057 \n",
            "Epoch: 235 | Loss: 0.006177821196615696 \n",
            "Epoch: 236 | Loss: 0.006089044734835625 \n",
            "Epoch: 237 | Loss: 0.006001550704240799 \n",
            "Epoch: 238 | Loss: 0.005915316287428141 \n",
            "Epoch: 239 | Loss: 0.005830308888107538 \n",
            "Epoch: 240 | Loss: 0.005746481940150261 \n",
            "Epoch: 241 | Loss: 0.0056639136746525764 \n",
            "Epoch: 242 | Loss: 0.005582485347986221 \n",
            "Epoch: 243 | Loss: 0.005502264015376568 \n",
            "Epoch: 244 | Loss: 0.005423175171017647 \n",
            "Epoch: 245 | Loss: 0.005345245823264122 \n",
            "Epoch: 246 | Loss: 0.005268433131277561 \n",
            "Epoch: 247 | Loss: 0.005192691460251808 \n",
            "Epoch: 248 | Loss: 0.00511807668954134 \n",
            "Epoch: 249 | Loss: 0.005044509656727314 \n",
            "Epoch: 250 | Loss: 0.004972033202648163 \n",
            "Epoch: 251 | Loss: 0.0049005732871592045 \n",
            "Epoch: 252 | Loss: 0.004830168094485998 \n",
            "Epoch: 253 | Loss: 0.004760762210935354 \n",
            "Epoch: 254 | Loss: 0.0046923160552978516 \n",
            "Epoch: 255 | Loss: 0.004624887835234404 \n",
            "Epoch: 256 | Loss: 0.004558430518954992 \n",
            "Epoch: 257 | Loss: 0.004492898005992174 \n",
            "Epoch: 258 | Loss: 0.004428358748555183 \n",
            "Epoch: 259 | Loss: 0.004364689812064171 \n",
            "Epoch: 260 | Loss: 0.0043019396252930164 \n",
            "Epoch: 261 | Loss: 0.0042401584796607494 \n",
            "Epoch: 262 | Loss: 0.004179203882813454 \n",
            "Epoch: 263 | Loss: 0.004119164310395718 \n",
            "Epoch: 264 | Loss: 0.004059929866343737 \n",
            "Epoch: 265 | Loss: 0.004001584369689226 \n",
            "Epoch: 266 | Loss: 0.003944073338061571 \n",
            "Epoch: 267 | Loss: 0.0038873883895576 \n",
            "Epoch: 268 | Loss: 0.003831538837403059 \n",
            "Epoch: 269 | Loss: 0.0037764643784612417 \n",
            "Epoch: 270 | Loss: 0.0037221936509013176 \n",
            "Epoch: 271 | Loss: 0.003668706165626645 \n",
            "Epoch: 272 | Loss: 0.0036159693263471127 \n",
            "Epoch: 273 | Loss: 0.0035640313290059566 \n",
            "Epoch: 274 | Loss: 0.003512782510370016 \n",
            "Epoch: 275 | Loss: 0.0034623146057128906 \n",
            "Epoch: 276 | Loss: 0.003412562655285001 \n",
            "Epoch: 277 | Loss: 0.0033635245636105537 \n",
            "Epoch: 278 | Loss: 0.003315185895189643 \n",
            "Epoch: 279 | Loss: 0.003267530119046569 \n",
            "Epoch: 280 | Loss: 0.0032205882016569376 \n",
            "Epoch: 281 | Loss: 0.003174267942085862 \n",
            "Epoch: 282 | Loss: 0.0031286573503166437 \n",
            "Epoch: 283 | Loss: 0.0030837124213576317 \n",
            "Epoch: 284 | Loss: 0.00303938752040267 \n",
            "Epoch: 285 | Loss: 0.002995708491653204 \n",
            "Epoch: 286 | Loss: 0.002952651586383581 \n",
            "Epoch: 287 | Loss: 0.0029102107509970665 \n",
            "Epoch: 288 | Loss: 0.002868382725864649 \n",
            "Epoch: 289 | Loss: 0.00282717845402658 \n",
            "Epoch: 290 | Loss: 0.00278655206784606 \n",
            "Epoch: 291 | Loss: 0.002746490528807044 \n",
            "Epoch: 292 | Loss: 0.0027070369105786085 \n",
            "Epoch: 293 | Loss: 0.0026681148447096348 \n",
            "Epoch: 294 | Loss: 0.002629755064845085 \n",
            "Epoch: 295 | Loss: 0.0025919717736542225 \n",
            "Epoch: 296 | Loss: 0.0025547235272824764 \n",
            "Epoch: 297 | Loss: 0.0025180019438266754 \n",
            "Epoch: 298 | Loss: 0.0024818123783916235 \n",
            "Epoch: 299 | Loss: 0.0024461608845740557 \n",
            "Epoch: 300 | Loss: 0.002410995541140437 \n",
            "Epoch: 301 | Loss: 0.002376345917582512 \n",
            "Epoch: 302 | Loss: 0.0023422057274729013 \n",
            "Epoch: 303 | Loss: 0.0023085500579327345 \n",
            "Epoch: 304 | Loss: 0.00227535143494606 \n",
            "Epoch: 305 | Loss: 0.002242663409560919 \n",
            "Epoch: 306 | Loss: 0.002210435690358281 \n",
            "Epoch: 307 | Loss: 0.0021786775905638933 \n",
            "Epoch: 308 | Loss: 0.002147356513887644 \n",
            "Epoch: 309 | Loss: 0.0021164845675230026 \n",
            "Epoch: 310 | Loss: 0.0020860880613327026 \n",
            "Epoch: 311 | Loss: 0.002056096214801073 \n",
            "Epoch: 312 | Loss: 0.0020265362691134214 \n",
            "Epoch: 313 | Loss: 0.001997437095269561 \n",
            "Epoch: 314 | Loss: 0.0019687211606651545 \n",
            "Epoch: 315 | Loss: 0.001940425718203187 \n",
            "Epoch: 316 | Loss: 0.0019125459948554635 \n",
            "Epoch: 317 | Loss: 0.001885055098682642 \n",
            "Epoch: 318 | Loss: 0.0018579460447654128 \n",
            "Epoch: 319 | Loss: 0.0018312648171558976 \n",
            "Epoch: 320 | Loss: 0.0018049345817416906 \n",
            "Epoch: 321 | Loss: 0.0017789985286071897 \n",
            "Epoch: 322 | Loss: 0.0017534157959744334 \n",
            "Epoch: 323 | Loss: 0.0017282237531617284 \n",
            "Epoch: 324 | Loss: 0.0017033820040524006 \n",
            "Epoch: 325 | Loss: 0.0016789059154689312 \n",
            "Epoch: 326 | Loss: 0.0016547773266211152 \n",
            "Epoch: 327 | Loss: 0.0016310017090290785 \n",
            "Epoch: 328 | Loss: 0.001607577782124281 \n",
            "Epoch: 329 | Loss: 0.001584473648108542 \n",
            "Epoch: 330 | Loss: 0.00156169303227216 \n",
            "Epoch: 331 | Loss: 0.0015392439672723413 \n",
            "Epoch: 332 | Loss: 0.0015171158593147993 \n",
            "Epoch: 333 | Loss: 0.0014953119680285454 \n",
            "Epoch: 334 | Loss: 0.0014738289173692465 \n",
            "Epoch: 335 | Loss: 0.0014526343438774347 \n",
            "Epoch: 336 | Loss: 0.0014317824970930815 \n",
            "Epoch: 337 | Loss: 0.001411188393831253 \n",
            "Epoch: 338 | Loss: 0.001390925608575344 \n",
            "Epoch: 339 | Loss: 0.0013709384948015213 \n",
            "Epoch: 340 | Loss: 0.0013512179721146822 \n",
            "Epoch: 341 | Loss: 0.001331803621724248 \n",
            "Epoch: 342 | Loss: 0.0013126539997756481 \n",
            "Epoch: 343 | Loss: 0.0012938021682202816 \n",
            "Epoch: 344 | Loss: 0.0012751924805343151 \n",
            "Epoch: 345 | Loss: 0.0012568720849230886 \n",
            "Epoch: 346 | Loss: 0.001238807337358594 \n",
            "Epoch: 347 | Loss: 0.001221013255417347 \n",
            "Epoch: 348 | Loss: 0.0012034650426357985 \n",
            "Epoch: 349 | Loss: 0.0011861578095704317 \n",
            "Epoch: 350 | Loss: 0.0011691171675920486 \n",
            "Epoch: 351 | Loss: 0.001152316341176629 \n",
            "Epoch: 352 | Loss: 0.0011357684852555394 \n",
            "Epoch: 353 | Loss: 0.0011194237740710378 \n",
            "Epoch: 354 | Loss: 0.0011033483315259218 \n",
            "Epoch: 355 | Loss: 0.0010875021107494831 \n",
            "Epoch: 356 | Loss: 0.0010718562407419086 \n",
            "Epoch: 357 | Loss: 0.0010564617114141583 \n",
            "Epoch: 358 | Loss: 0.001041279872879386 \n",
            "Epoch: 359 | Loss: 0.0010263047879561782 \n",
            "Epoch: 360 | Loss: 0.0010115564800798893 \n",
            "Epoch: 361 | Loss: 0.0009970195824280381 \n",
            "Epoch: 362 | Loss: 0.0009826917666941881 \n",
            "Epoch: 363 | Loss: 0.0009685816476121545 \n",
            "Epoch: 364 | Loss: 0.0009546454530209303 \n",
            "Epoch: 365 | Loss: 0.0009409349877387285 \n",
            "Epoch: 366 | Loss: 0.000927419401705265 \n",
            "Epoch: 367 | Loss: 0.0009140877518802881 \n",
            "Epoch: 368 | Loss: 0.0009009311906993389 \n",
            "Epoch: 369 | Loss: 0.0008879894157871604 \n",
            "Epoch: 370 | Loss: 0.000875232566613704 \n",
            "Epoch: 371 | Loss: 0.000862654997035861 \n",
            "Epoch: 372 | Loss: 0.000850255019031465 \n",
            "Epoch: 373 | Loss: 0.0008380339713767171 \n",
            "Epoch: 374 | Loss: 0.0008259916212409735 \n",
            "Epoch: 375 | Loss: 0.0008141326834447682 \n",
            "Epoch: 376 | Loss: 0.0008024138514883816 \n",
            "Epoch: 377 | Loss: 0.0007908829720690846 \n",
            "Epoch: 378 | Loss: 0.000779524736572057 \n",
            "Epoch: 379 | Loss: 0.0007683197036385536 \n",
            "Epoch: 380 | Loss: 0.0007572805625386536 \n",
            "Epoch: 381 | Loss: 0.0007464038790203631 \n",
            "Epoch: 382 | Loss: 0.0007356752757914364 \n",
            "Epoch: 383 | Loss: 0.0007250916678458452 \n",
            "Epoch: 384 | Loss: 0.0007146717398427427 \n",
            "Epoch: 385 | Loss: 0.000704405945725739 \n",
            "Epoch: 386 | Loss: 0.0006942696636542678 \n",
            "Epoch: 387 | Loss: 0.0006843116716481745 \n",
            "Epoch: 388 | Loss: 0.0006744724814780056 \n",
            "Epoch: 389 | Loss: 0.0006647674599662423 \n",
            "Epoch: 390 | Loss: 0.0006552266422659159 \n",
            "Epoch: 391 | Loss: 0.0006458064308390021 \n",
            "Epoch: 392 | Loss: 0.0006365189328789711 \n",
            "Epoch: 393 | Loss: 0.000627368688583374 \n",
            "Epoch: 394 | Loss: 0.0006183554651215672 \n",
            "Epoch: 395 | Loss: 0.0006094620330259204 \n",
            "Epoch: 396 | Loss: 0.0006007113843224943 \n",
            "Epoch: 397 | Loss: 0.0005920735420659184 \n",
            "Epoch: 398 | Loss: 0.0005835698684677482 \n",
            "Epoch: 399 | Loss: 0.0005751821445301175 \n",
            "Epoch: 400 | Loss: 0.0005669090896844864 \n",
            "Epoch: 401 | Loss: 0.000558764673769474 \n",
            "Epoch: 402 | Loss: 0.000550729688256979 \n",
            "Epoch: 403 | Loss: 0.0005428152508102357 \n",
            "Epoch: 404 | Loss: 0.0005350104765966535 \n",
            "Epoch: 405 | Loss: 0.000527337018866092 \n",
            "Epoch: 406 | Loss: 0.0005197534919716418 \n",
            "Epoch: 407 | Loss: 0.000512282713316381 \n",
            "Epoch: 408 | Loss: 0.0005049222381785512 \n",
            "Epoch: 409 | Loss: 0.0004976630443707108 \n",
            "Epoch: 410 | Loss: 0.0004905195091851056 \n",
            "Epoch: 411 | Loss: 0.0004834634019061923 \n",
            "Epoch: 412 | Loss: 0.0004765041230712086 \n",
            "Epoch: 413 | Loss: 0.0004696708347182721 \n",
            "Epoch: 414 | Loss: 0.00046292447950690985 \n",
            "Epoch: 415 | Loss: 0.000456262961961329 \n",
            "Epoch: 416 | Loss: 0.00044970642193220556 \n",
            "Epoch: 417 | Loss: 0.00044325238559395075 \n",
            "Epoch: 418 | Loss: 0.00043687300058081746 \n",
            "Epoch: 419 | Loss: 0.000430601357948035 \n",
            "Epoch: 420 | Loss: 0.0004244134179316461 \n",
            "Epoch: 421 | Loss: 0.0004183035343885422 \n",
            "Epoch: 422 | Loss: 0.00041230052011087537 \n",
            "Epoch: 423 | Loss: 0.00040637608617544174 \n",
            "Epoch: 424 | Loss: 0.0004005271184723824 \n",
            "Epoch: 425 | Loss: 0.00039477599784731865 \n",
            "Epoch: 426 | Loss: 0.00038910770672373474 \n",
            "Epoch: 427 | Loss: 0.0003835134266410023 \n",
            "Epoch: 428 | Loss: 0.000377991353161633 \n",
            "Epoch: 429 | Loss: 0.00037255644565448165 \n",
            "Epoch: 430 | Loss: 0.0003672065504360944 \n",
            "Epoch: 431 | Loss: 0.00036192304105497897 \n",
            "Epoch: 432 | Loss: 0.0003567337989807129 \n",
            "Epoch: 433 | Loss: 0.00035159947583451867 \n",
            "Epoch: 434 | Loss: 0.0003465500194579363 \n",
            "Epoch: 435 | Loss: 0.00034157573827542365 \n",
            "Epoch: 436 | Loss: 0.0003366640885360539 \n",
            "Epoch: 437 | Loss: 0.00033182825427502394 \n",
            "Epoch: 438 | Loss: 0.0003270474262535572 \n",
            "Epoch: 439 | Loss: 0.00032235862454399467 \n",
            "Epoch: 440 | Loss: 0.00031772549846209586 \n",
            "Epoch: 441 | Loss: 0.00031315471278503537 \n",
            "Epoch: 442 | Loss: 0.0003086579090449959 \n",
            "Epoch: 443 | Loss: 0.0003042179741896689 \n",
            "Epoch: 444 | Loss: 0.0002998464333359152 \n",
            "Epoch: 445 | Loss: 0.0002955435193143785 \n",
            "Epoch: 446 | Loss: 0.0002912925265263766 \n",
            "Epoch: 447 | Loss: 0.0002871087926905602 \n",
            "Epoch: 448 | Loss: 0.0002829739241860807 \n",
            "Epoch: 449 | Loss: 0.00027890881756320596 \n",
            "Epoch: 450 | Loss: 0.00027490206412039697 \n",
            "Epoch: 451 | Loss: 0.00027094921097159386 \n",
            "Epoch: 452 | Loss: 0.0002670572721399367 \n",
            "Epoch: 453 | Loss: 0.0002632236573845148 \n",
            "Epoch: 454 | Loss: 0.0002594374236650765 \n",
            "Epoch: 455 | Loss: 0.00025570561410859227 \n",
            "Epoch: 456 | Loss: 0.00025203771656379104 \n",
            "Epoch: 457 | Loss: 0.00024841559934429824 \n",
            "Epoch: 458 | Loss: 0.00024484790628775954 \n",
            "Epoch: 459 | Loss: 0.00024132317048497498 \n",
            "Epoch: 460 | Loss: 0.00023785358644090593 \n",
            "Epoch: 461 | Loss: 0.00023442959354724735 \n",
            "Epoch: 462 | Loss: 0.0002310675336048007 \n",
            "Epoch: 463 | Loss: 0.0002277517196489498 \n",
            "Epoch: 464 | Loss: 0.00022447813535109162 \n",
            "Epoch: 465 | Loss: 0.00022124643146526068 \n",
            "Epoch: 466 | Loss: 0.00021807249868288636 \n",
            "Epoch: 467 | Loss: 0.00021493341773748398 \n",
            "Epoch: 468 | Loss: 0.00021184756769798696 \n",
            "Epoch: 469 | Loss: 0.0002087983302772045 \n",
            "Epoch: 470 | Loss: 0.00020579539705067873 \n",
            "Epoch: 471 | Loss: 0.00020284729544073343 \n",
            "Epoch: 472 | Loss: 0.00019992295710835606 \n",
            "Epoch: 473 | Loss: 0.0001970533048734069 \n",
            "Epoch: 474 | Loss: 0.00019422061450313777 \n",
            "Epoch: 475 | Loss: 0.00019142610835842788 \n",
            "Epoch: 476 | Loss: 0.00018868381448555738 \n",
            "Epoch: 477 | Loss: 0.0001859724143287167 \n",
            "Epoch: 478 | Loss: 0.00018329166050534695 \n",
            "Epoch: 479 | Loss: 0.00018065847689285874 \n",
            "Epoch: 480 | Loss: 0.00017806378309614956 \n",
            "Epoch: 481 | Loss: 0.0001755109115038067 \n",
            "Epoch: 482 | Loss: 0.00017298033344559371 \n",
            "Epoch: 483 | Loss: 0.00017049850430339575 \n",
            "Epoch: 484 | Loss: 0.0001680451532592997 \n",
            "Epoch: 485 | Loss: 0.00016563200915697962 \n",
            "Epoch: 486 | Loss: 0.0001632467465242371 \n",
            "Epoch: 487 | Loss: 0.00016090826829895377 \n",
            "Epoch: 488 | Loss: 0.00015859032282605767 \n",
            "Epoch: 489 | Loss: 0.0001563110126880929 \n",
            "Epoch: 490 | Loss: 0.00015406985767185688 \n",
            "Epoch: 491 | Loss: 0.00015185131633188576 \n",
            "Epoch: 492 | Loss: 0.00014966950402595103 \n",
            "Epoch: 493 | Loss: 0.00014751900744158775 \n",
            "Epoch: 494 | Loss: 0.00014540091797243804 \n",
            "Epoch: 495 | Loss: 0.00014331203419715166 \n",
            "Epoch: 496 | Loss: 0.00014124934386927634 \n",
            "Epoch: 497 | Loss: 0.0001392221893183887 \n",
            "Epoch: 498 | Loss: 0.00013721376308239996 \n",
            "Epoch: 499 | Loss: 0.00013524698442779481 \n",
            "Prediction (after training) 4 7.986631870269775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Logistic Regression\n",
        "\n",
        ": find w that minimize the loss\n",
        "\n",
        "-> **argmin**(or **max**)loss(w)"
      ],
      "metadata": {
        "id": "Q9NNAzy7SFTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import sigmoid"
      ],
      "metadata": {
        "id": "phOb8fIHTbDj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data and ground truth\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])"
      ],
      "metadata": {
        "id": "drwHuhiOSFfR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data.\n",
        "        \"\"\"\n",
        "        y_pred = sigmoid(self.linear(x))\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "BufF017EIV2U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo5L7AEmIcId",
        "outputId": "77bfe58e-d44f-47b0-cffb-43c20c3b0cab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 | Loss: 0.7239\n",
            "Epoch 2/1000 | Loss: 0.7233\n",
            "Epoch 3/1000 | Loss: 0.7227\n",
            "Epoch 4/1000 | Loss: 0.7222\n",
            "Epoch 5/1000 | Loss: 0.7216\n",
            "Epoch 6/1000 | Loss: 0.7211\n",
            "Epoch 7/1000 | Loss: 0.7205\n",
            "Epoch 8/1000 | Loss: 0.7200\n",
            "Epoch 9/1000 | Loss: 0.7194\n",
            "Epoch 10/1000 | Loss: 0.7189\n",
            "Epoch 11/1000 | Loss: 0.7184\n",
            "Epoch 12/1000 | Loss: 0.7179\n",
            "Epoch 13/1000 | Loss: 0.7174\n",
            "Epoch 14/1000 | Loss: 0.7169\n",
            "Epoch 15/1000 | Loss: 0.7164\n",
            "Epoch 16/1000 | Loss: 0.7159\n",
            "Epoch 17/1000 | Loss: 0.7154\n",
            "Epoch 18/1000 | Loss: 0.7150\n",
            "Epoch 19/1000 | Loss: 0.7145\n",
            "Epoch 20/1000 | Loss: 0.7140\n",
            "Epoch 21/1000 | Loss: 0.7136\n",
            "Epoch 22/1000 | Loss: 0.7131\n",
            "Epoch 23/1000 | Loss: 0.7127\n",
            "Epoch 24/1000 | Loss: 0.7122\n",
            "Epoch 25/1000 | Loss: 0.7118\n",
            "Epoch 26/1000 | Loss: 0.7114\n",
            "Epoch 27/1000 | Loss: 0.7109\n",
            "Epoch 28/1000 | Loss: 0.7105\n",
            "Epoch 29/1000 | Loss: 0.7101\n",
            "Epoch 30/1000 | Loss: 0.7097\n",
            "Epoch 31/1000 | Loss: 0.7093\n",
            "Epoch 32/1000 | Loss: 0.7088\n",
            "Epoch 33/1000 | Loss: 0.7084\n",
            "Epoch 34/1000 | Loss: 0.7080\n",
            "Epoch 35/1000 | Loss: 0.7076\n",
            "Epoch 36/1000 | Loss: 0.7072\n",
            "Epoch 37/1000 | Loss: 0.7068\n",
            "Epoch 38/1000 | Loss: 0.7065\n",
            "Epoch 39/1000 | Loss: 0.7061\n",
            "Epoch 40/1000 | Loss: 0.7057\n",
            "Epoch 41/1000 | Loss: 0.7053\n",
            "Epoch 42/1000 | Loss: 0.7049\n",
            "Epoch 43/1000 | Loss: 0.7045\n",
            "Epoch 44/1000 | Loss: 0.7042\n",
            "Epoch 45/1000 | Loss: 0.7038\n",
            "Epoch 46/1000 | Loss: 0.7034\n",
            "Epoch 47/1000 | Loss: 0.7031\n",
            "Epoch 48/1000 | Loss: 0.7027\n",
            "Epoch 49/1000 | Loss: 0.7023\n",
            "Epoch 50/1000 | Loss: 0.7020\n",
            "Epoch 51/1000 | Loss: 0.7016\n",
            "Epoch 52/1000 | Loss: 0.7013\n",
            "Epoch 53/1000 | Loss: 0.7009\n",
            "Epoch 54/1000 | Loss: 0.7005\n",
            "Epoch 55/1000 | Loss: 0.7002\n",
            "Epoch 56/1000 | Loss: 0.6998\n",
            "Epoch 57/1000 | Loss: 0.6995\n",
            "Epoch 58/1000 | Loss: 0.6991\n",
            "Epoch 59/1000 | Loss: 0.6988\n",
            "Epoch 60/1000 | Loss: 0.6985\n",
            "Epoch 61/1000 | Loss: 0.6981\n",
            "Epoch 62/1000 | Loss: 0.6978\n",
            "Epoch 63/1000 | Loss: 0.6974\n",
            "Epoch 64/1000 | Loss: 0.6971\n",
            "Epoch 65/1000 | Loss: 0.6968\n",
            "Epoch 66/1000 | Loss: 0.6964\n",
            "Epoch 67/1000 | Loss: 0.6961\n",
            "Epoch 68/1000 | Loss: 0.6958\n",
            "Epoch 69/1000 | Loss: 0.6954\n",
            "Epoch 70/1000 | Loss: 0.6951\n",
            "Epoch 71/1000 | Loss: 0.6948\n",
            "Epoch 72/1000 | Loss: 0.6945\n",
            "Epoch 73/1000 | Loss: 0.6941\n",
            "Epoch 74/1000 | Loss: 0.6938\n",
            "Epoch 75/1000 | Loss: 0.6935\n",
            "Epoch 76/1000 | Loss: 0.6932\n",
            "Epoch 77/1000 | Loss: 0.6928\n",
            "Epoch 78/1000 | Loss: 0.6925\n",
            "Epoch 79/1000 | Loss: 0.6922\n",
            "Epoch 80/1000 | Loss: 0.6919\n",
            "Epoch 81/1000 | Loss: 0.6916\n",
            "Epoch 82/1000 | Loss: 0.6912\n",
            "Epoch 83/1000 | Loss: 0.6909\n",
            "Epoch 84/1000 | Loss: 0.6906\n",
            "Epoch 85/1000 | Loss: 0.6903\n",
            "Epoch 86/1000 | Loss: 0.6900\n",
            "Epoch 87/1000 | Loss: 0.6897\n",
            "Epoch 88/1000 | Loss: 0.6893\n",
            "Epoch 89/1000 | Loss: 0.6890\n",
            "Epoch 90/1000 | Loss: 0.6887\n",
            "Epoch 91/1000 | Loss: 0.6884\n",
            "Epoch 92/1000 | Loss: 0.6881\n",
            "Epoch 93/1000 | Loss: 0.6878\n",
            "Epoch 94/1000 | Loss: 0.6875\n",
            "Epoch 95/1000 | Loss: 0.6872\n",
            "Epoch 96/1000 | Loss: 0.6869\n",
            "Epoch 97/1000 | Loss: 0.6866\n",
            "Epoch 98/1000 | Loss: 0.6863\n",
            "Epoch 99/1000 | Loss: 0.6859\n",
            "Epoch 100/1000 | Loss: 0.6856\n",
            "Epoch 101/1000 | Loss: 0.6853\n",
            "Epoch 102/1000 | Loss: 0.6850\n",
            "Epoch 103/1000 | Loss: 0.6847\n",
            "Epoch 104/1000 | Loss: 0.6844\n",
            "Epoch 105/1000 | Loss: 0.6841\n",
            "Epoch 106/1000 | Loss: 0.6838\n",
            "Epoch 107/1000 | Loss: 0.6835\n",
            "Epoch 108/1000 | Loss: 0.6832\n",
            "Epoch 109/1000 | Loss: 0.6829\n",
            "Epoch 110/1000 | Loss: 0.6826\n",
            "Epoch 111/1000 | Loss: 0.6823\n",
            "Epoch 112/1000 | Loss: 0.6820\n",
            "Epoch 113/1000 | Loss: 0.6817\n",
            "Epoch 114/1000 | Loss: 0.6814\n",
            "Epoch 115/1000 | Loss: 0.6811\n",
            "Epoch 116/1000 | Loss: 0.6808\n",
            "Epoch 117/1000 | Loss: 0.6805\n",
            "Epoch 118/1000 | Loss: 0.6802\n",
            "Epoch 119/1000 | Loss: 0.6799\n",
            "Epoch 120/1000 | Loss: 0.6796\n",
            "Epoch 121/1000 | Loss: 0.6793\n",
            "Epoch 122/1000 | Loss: 0.6790\n",
            "Epoch 123/1000 | Loss: 0.6787\n",
            "Epoch 124/1000 | Loss: 0.6784\n",
            "Epoch 125/1000 | Loss: 0.6781\n",
            "Epoch 126/1000 | Loss: 0.6778\n",
            "Epoch 127/1000 | Loss: 0.6776\n",
            "Epoch 128/1000 | Loss: 0.6773\n",
            "Epoch 129/1000 | Loss: 0.6770\n",
            "Epoch 130/1000 | Loss: 0.6767\n",
            "Epoch 131/1000 | Loss: 0.6764\n",
            "Epoch 132/1000 | Loss: 0.6761\n",
            "Epoch 133/1000 | Loss: 0.6758\n",
            "Epoch 134/1000 | Loss: 0.6755\n",
            "Epoch 135/1000 | Loss: 0.6752\n",
            "Epoch 136/1000 | Loss: 0.6749\n",
            "Epoch 137/1000 | Loss: 0.6746\n",
            "Epoch 138/1000 | Loss: 0.6743\n",
            "Epoch 139/1000 | Loss: 0.6740\n",
            "Epoch 140/1000 | Loss: 0.6738\n",
            "Epoch 141/1000 | Loss: 0.6735\n",
            "Epoch 142/1000 | Loss: 0.6732\n",
            "Epoch 143/1000 | Loss: 0.6729\n",
            "Epoch 144/1000 | Loss: 0.6726\n",
            "Epoch 145/1000 | Loss: 0.6723\n",
            "Epoch 146/1000 | Loss: 0.6720\n",
            "Epoch 147/1000 | Loss: 0.6717\n",
            "Epoch 148/1000 | Loss: 0.6714\n",
            "Epoch 149/1000 | Loss: 0.6712\n",
            "Epoch 150/1000 | Loss: 0.6709\n",
            "Epoch 151/1000 | Loss: 0.6706\n",
            "Epoch 152/1000 | Loss: 0.6703\n",
            "Epoch 153/1000 | Loss: 0.6700\n",
            "Epoch 154/1000 | Loss: 0.6697\n",
            "Epoch 155/1000 | Loss: 0.6694\n",
            "Epoch 156/1000 | Loss: 0.6691\n",
            "Epoch 157/1000 | Loss: 0.6689\n",
            "Epoch 158/1000 | Loss: 0.6686\n",
            "Epoch 159/1000 | Loss: 0.6683\n",
            "Epoch 160/1000 | Loss: 0.6680\n",
            "Epoch 161/1000 | Loss: 0.6677\n",
            "Epoch 162/1000 | Loss: 0.6674\n",
            "Epoch 163/1000 | Loss: 0.6671\n",
            "Epoch 164/1000 | Loss: 0.6669\n",
            "Epoch 165/1000 | Loss: 0.6666\n",
            "Epoch 166/1000 | Loss: 0.6663\n",
            "Epoch 167/1000 | Loss: 0.6660\n",
            "Epoch 168/1000 | Loss: 0.6657\n",
            "Epoch 169/1000 | Loss: 0.6654\n",
            "Epoch 170/1000 | Loss: 0.6652\n",
            "Epoch 171/1000 | Loss: 0.6649\n",
            "Epoch 172/1000 | Loss: 0.6646\n",
            "Epoch 173/1000 | Loss: 0.6643\n",
            "Epoch 174/1000 | Loss: 0.6640\n",
            "Epoch 175/1000 | Loss: 0.6637\n",
            "Epoch 176/1000 | Loss: 0.6635\n",
            "Epoch 177/1000 | Loss: 0.6632\n",
            "Epoch 178/1000 | Loss: 0.6629\n",
            "Epoch 179/1000 | Loss: 0.6626\n",
            "Epoch 180/1000 | Loss: 0.6623\n",
            "Epoch 181/1000 | Loss: 0.6621\n",
            "Epoch 182/1000 | Loss: 0.6618\n",
            "Epoch 183/1000 | Loss: 0.6615\n",
            "Epoch 184/1000 | Loss: 0.6612\n",
            "Epoch 185/1000 | Loss: 0.6609\n",
            "Epoch 186/1000 | Loss: 0.6607\n",
            "Epoch 187/1000 | Loss: 0.6604\n",
            "Epoch 188/1000 | Loss: 0.6601\n",
            "Epoch 189/1000 | Loss: 0.6598\n",
            "Epoch 190/1000 | Loss: 0.6595\n",
            "Epoch 191/1000 | Loss: 0.6593\n",
            "Epoch 192/1000 | Loss: 0.6590\n",
            "Epoch 193/1000 | Loss: 0.6587\n",
            "Epoch 194/1000 | Loss: 0.6584\n",
            "Epoch 195/1000 | Loss: 0.6582\n",
            "Epoch 196/1000 | Loss: 0.6579\n",
            "Epoch 197/1000 | Loss: 0.6576\n",
            "Epoch 198/1000 | Loss: 0.6573\n",
            "Epoch 199/1000 | Loss: 0.6570\n",
            "Epoch 200/1000 | Loss: 0.6568\n",
            "Epoch 201/1000 | Loss: 0.6565\n",
            "Epoch 202/1000 | Loss: 0.6562\n",
            "Epoch 203/1000 | Loss: 0.6559\n",
            "Epoch 204/1000 | Loss: 0.6557\n",
            "Epoch 205/1000 | Loss: 0.6554\n",
            "Epoch 206/1000 | Loss: 0.6551\n",
            "Epoch 207/1000 | Loss: 0.6548\n",
            "Epoch 208/1000 | Loss: 0.6546\n",
            "Epoch 209/1000 | Loss: 0.6543\n",
            "Epoch 210/1000 | Loss: 0.6540\n",
            "Epoch 211/1000 | Loss: 0.6537\n",
            "Epoch 212/1000 | Loss: 0.6535\n",
            "Epoch 213/1000 | Loss: 0.6532\n",
            "Epoch 214/1000 | Loss: 0.6529\n",
            "Epoch 215/1000 | Loss: 0.6526\n",
            "Epoch 216/1000 | Loss: 0.6524\n",
            "Epoch 217/1000 | Loss: 0.6521\n",
            "Epoch 218/1000 | Loss: 0.6518\n",
            "Epoch 219/1000 | Loss: 0.6515\n",
            "Epoch 220/1000 | Loss: 0.6513\n",
            "Epoch 221/1000 | Loss: 0.6510\n",
            "Epoch 222/1000 | Loss: 0.6507\n",
            "Epoch 223/1000 | Loss: 0.6505\n",
            "Epoch 224/1000 | Loss: 0.6502\n",
            "Epoch 225/1000 | Loss: 0.6499\n",
            "Epoch 226/1000 | Loss: 0.6496\n",
            "Epoch 227/1000 | Loss: 0.6494\n",
            "Epoch 228/1000 | Loss: 0.6491\n",
            "Epoch 229/1000 | Loss: 0.6488\n",
            "Epoch 230/1000 | Loss: 0.6486\n",
            "Epoch 231/1000 | Loss: 0.6483\n",
            "Epoch 232/1000 | Loss: 0.6480\n",
            "Epoch 233/1000 | Loss: 0.6477\n",
            "Epoch 234/1000 | Loss: 0.6475\n",
            "Epoch 235/1000 | Loss: 0.6472\n",
            "Epoch 236/1000 | Loss: 0.6469\n",
            "Epoch 237/1000 | Loss: 0.6467\n",
            "Epoch 238/1000 | Loss: 0.6464\n",
            "Epoch 239/1000 | Loss: 0.6461\n",
            "Epoch 240/1000 | Loss: 0.6459\n",
            "Epoch 241/1000 | Loss: 0.6456\n",
            "Epoch 242/1000 | Loss: 0.6453\n",
            "Epoch 243/1000 | Loss: 0.6450\n",
            "Epoch 244/1000 | Loss: 0.6448\n",
            "Epoch 245/1000 | Loss: 0.6445\n",
            "Epoch 246/1000 | Loss: 0.6442\n",
            "Epoch 247/1000 | Loss: 0.6440\n",
            "Epoch 248/1000 | Loss: 0.6437\n",
            "Epoch 249/1000 | Loss: 0.6434\n",
            "Epoch 250/1000 | Loss: 0.6432\n",
            "Epoch 251/1000 | Loss: 0.6429\n",
            "Epoch 252/1000 | Loss: 0.6426\n",
            "Epoch 253/1000 | Loss: 0.6424\n",
            "Epoch 254/1000 | Loss: 0.6421\n",
            "Epoch 255/1000 | Loss: 0.6418\n",
            "Epoch 256/1000 | Loss: 0.6416\n",
            "Epoch 257/1000 | Loss: 0.6413\n",
            "Epoch 258/1000 | Loss: 0.6410\n",
            "Epoch 259/1000 | Loss: 0.6408\n",
            "Epoch 260/1000 | Loss: 0.6405\n",
            "Epoch 261/1000 | Loss: 0.6402\n",
            "Epoch 262/1000 | Loss: 0.6400\n",
            "Epoch 263/1000 | Loss: 0.6397\n",
            "Epoch 264/1000 | Loss: 0.6395\n",
            "Epoch 265/1000 | Loss: 0.6392\n",
            "Epoch 266/1000 | Loss: 0.6389\n",
            "Epoch 267/1000 | Loss: 0.6387\n",
            "Epoch 268/1000 | Loss: 0.6384\n",
            "Epoch 269/1000 | Loss: 0.6381\n",
            "Epoch 270/1000 | Loss: 0.6379\n",
            "Epoch 271/1000 | Loss: 0.6376\n",
            "Epoch 272/1000 | Loss: 0.6373\n",
            "Epoch 273/1000 | Loss: 0.6371\n",
            "Epoch 274/1000 | Loss: 0.6368\n",
            "Epoch 275/1000 | Loss: 0.6366\n",
            "Epoch 276/1000 | Loss: 0.6363\n",
            "Epoch 277/1000 | Loss: 0.6360\n",
            "Epoch 278/1000 | Loss: 0.6358\n",
            "Epoch 279/1000 | Loss: 0.6355\n",
            "Epoch 280/1000 | Loss: 0.6352\n",
            "Epoch 281/1000 | Loss: 0.6350\n",
            "Epoch 282/1000 | Loss: 0.6347\n",
            "Epoch 283/1000 | Loss: 0.6345\n",
            "Epoch 284/1000 | Loss: 0.6342\n",
            "Epoch 285/1000 | Loss: 0.6339\n",
            "Epoch 286/1000 | Loss: 0.6337\n",
            "Epoch 287/1000 | Loss: 0.6334\n",
            "Epoch 288/1000 | Loss: 0.6332\n",
            "Epoch 289/1000 | Loss: 0.6329\n",
            "Epoch 290/1000 | Loss: 0.6326\n",
            "Epoch 291/1000 | Loss: 0.6324\n",
            "Epoch 292/1000 | Loss: 0.6321\n",
            "Epoch 293/1000 | Loss: 0.6319\n",
            "Epoch 294/1000 | Loss: 0.6316\n",
            "Epoch 295/1000 | Loss: 0.6313\n",
            "Epoch 296/1000 | Loss: 0.6311\n",
            "Epoch 297/1000 | Loss: 0.6308\n",
            "Epoch 298/1000 | Loss: 0.6306\n",
            "Epoch 299/1000 | Loss: 0.6303\n",
            "Epoch 300/1000 | Loss: 0.6301\n",
            "Epoch 301/1000 | Loss: 0.6298\n",
            "Epoch 302/1000 | Loss: 0.6295\n",
            "Epoch 303/1000 | Loss: 0.6293\n",
            "Epoch 304/1000 | Loss: 0.6290\n",
            "Epoch 305/1000 | Loss: 0.6288\n",
            "Epoch 306/1000 | Loss: 0.6285\n",
            "Epoch 307/1000 | Loss: 0.6283\n",
            "Epoch 308/1000 | Loss: 0.6280\n",
            "Epoch 309/1000 | Loss: 0.6277\n",
            "Epoch 310/1000 | Loss: 0.6275\n",
            "Epoch 311/1000 | Loss: 0.6272\n",
            "Epoch 312/1000 | Loss: 0.6270\n",
            "Epoch 313/1000 | Loss: 0.6267\n",
            "Epoch 314/1000 | Loss: 0.6265\n",
            "Epoch 315/1000 | Loss: 0.6262\n",
            "Epoch 316/1000 | Loss: 0.6260\n",
            "Epoch 317/1000 | Loss: 0.6257\n",
            "Epoch 318/1000 | Loss: 0.6254\n",
            "Epoch 319/1000 | Loss: 0.6252\n",
            "Epoch 320/1000 | Loss: 0.6249\n",
            "Epoch 321/1000 | Loss: 0.6247\n",
            "Epoch 322/1000 | Loss: 0.6244\n",
            "Epoch 323/1000 | Loss: 0.6242\n",
            "Epoch 324/1000 | Loss: 0.6239\n",
            "Epoch 325/1000 | Loss: 0.6237\n",
            "Epoch 326/1000 | Loss: 0.6234\n",
            "Epoch 327/1000 | Loss: 0.6232\n",
            "Epoch 328/1000 | Loss: 0.6229\n",
            "Epoch 329/1000 | Loss: 0.6227\n",
            "Epoch 330/1000 | Loss: 0.6224\n",
            "Epoch 331/1000 | Loss: 0.6221\n",
            "Epoch 332/1000 | Loss: 0.6219\n",
            "Epoch 333/1000 | Loss: 0.6216\n",
            "Epoch 334/1000 | Loss: 0.6214\n",
            "Epoch 335/1000 | Loss: 0.6211\n",
            "Epoch 336/1000 | Loss: 0.6209\n",
            "Epoch 337/1000 | Loss: 0.6206\n",
            "Epoch 338/1000 | Loss: 0.6204\n",
            "Epoch 339/1000 | Loss: 0.6201\n",
            "Epoch 340/1000 | Loss: 0.6199\n",
            "Epoch 341/1000 | Loss: 0.6196\n",
            "Epoch 342/1000 | Loss: 0.6194\n",
            "Epoch 343/1000 | Loss: 0.6191\n",
            "Epoch 344/1000 | Loss: 0.6189\n",
            "Epoch 345/1000 | Loss: 0.6186\n",
            "Epoch 346/1000 | Loss: 0.6184\n",
            "Epoch 347/1000 | Loss: 0.6181\n",
            "Epoch 348/1000 | Loss: 0.6179\n",
            "Epoch 349/1000 | Loss: 0.6176\n",
            "Epoch 350/1000 | Loss: 0.6174\n",
            "Epoch 351/1000 | Loss: 0.6171\n",
            "Epoch 352/1000 | Loss: 0.6169\n",
            "Epoch 353/1000 | Loss: 0.6166\n",
            "Epoch 354/1000 | Loss: 0.6164\n",
            "Epoch 355/1000 | Loss: 0.6161\n",
            "Epoch 356/1000 | Loss: 0.6159\n",
            "Epoch 357/1000 | Loss: 0.6156\n",
            "Epoch 358/1000 | Loss: 0.6154\n",
            "Epoch 359/1000 | Loss: 0.6152\n",
            "Epoch 360/1000 | Loss: 0.6149\n",
            "Epoch 361/1000 | Loss: 0.6147\n",
            "Epoch 362/1000 | Loss: 0.6144\n",
            "Epoch 363/1000 | Loss: 0.6142\n",
            "Epoch 364/1000 | Loss: 0.6139\n",
            "Epoch 365/1000 | Loss: 0.6137\n",
            "Epoch 366/1000 | Loss: 0.6134\n",
            "Epoch 367/1000 | Loss: 0.6132\n",
            "Epoch 368/1000 | Loss: 0.6129\n",
            "Epoch 369/1000 | Loss: 0.6127\n",
            "Epoch 370/1000 | Loss: 0.6124\n",
            "Epoch 371/1000 | Loss: 0.6122\n",
            "Epoch 372/1000 | Loss: 0.6119\n",
            "Epoch 373/1000 | Loss: 0.6117\n",
            "Epoch 374/1000 | Loss: 0.6115\n",
            "Epoch 375/1000 | Loss: 0.6112\n",
            "Epoch 376/1000 | Loss: 0.6110\n",
            "Epoch 377/1000 | Loss: 0.6107\n",
            "Epoch 378/1000 | Loss: 0.6105\n",
            "Epoch 379/1000 | Loss: 0.6102\n",
            "Epoch 380/1000 | Loss: 0.6100\n",
            "Epoch 381/1000 | Loss: 0.6097\n",
            "Epoch 382/1000 | Loss: 0.6095\n",
            "Epoch 383/1000 | Loss: 0.6093\n",
            "Epoch 384/1000 | Loss: 0.6090\n",
            "Epoch 385/1000 | Loss: 0.6088\n",
            "Epoch 386/1000 | Loss: 0.6085\n",
            "Epoch 387/1000 | Loss: 0.6083\n",
            "Epoch 388/1000 | Loss: 0.6080\n",
            "Epoch 389/1000 | Loss: 0.6078\n",
            "Epoch 390/1000 | Loss: 0.6076\n",
            "Epoch 391/1000 | Loss: 0.6073\n",
            "Epoch 392/1000 | Loss: 0.6071\n",
            "Epoch 393/1000 | Loss: 0.6068\n",
            "Epoch 394/1000 | Loss: 0.6066\n",
            "Epoch 395/1000 | Loss: 0.6064\n",
            "Epoch 396/1000 | Loss: 0.6061\n",
            "Epoch 397/1000 | Loss: 0.6059\n",
            "Epoch 398/1000 | Loss: 0.6056\n",
            "Epoch 399/1000 | Loss: 0.6054\n",
            "Epoch 400/1000 | Loss: 0.6052\n",
            "Epoch 401/1000 | Loss: 0.6049\n",
            "Epoch 402/1000 | Loss: 0.6047\n",
            "Epoch 403/1000 | Loss: 0.6044\n",
            "Epoch 404/1000 | Loss: 0.6042\n",
            "Epoch 405/1000 | Loss: 0.6040\n",
            "Epoch 406/1000 | Loss: 0.6037\n",
            "Epoch 407/1000 | Loss: 0.6035\n",
            "Epoch 408/1000 | Loss: 0.6032\n",
            "Epoch 409/1000 | Loss: 0.6030\n",
            "Epoch 410/1000 | Loss: 0.6028\n",
            "Epoch 411/1000 | Loss: 0.6025\n",
            "Epoch 412/1000 | Loss: 0.6023\n",
            "Epoch 413/1000 | Loss: 0.6020\n",
            "Epoch 414/1000 | Loss: 0.6018\n",
            "Epoch 415/1000 | Loss: 0.6016\n",
            "Epoch 416/1000 | Loss: 0.6013\n",
            "Epoch 417/1000 | Loss: 0.6011\n",
            "Epoch 418/1000 | Loss: 0.6009\n",
            "Epoch 419/1000 | Loss: 0.6006\n",
            "Epoch 420/1000 | Loss: 0.6004\n",
            "Epoch 421/1000 | Loss: 0.6001\n",
            "Epoch 422/1000 | Loss: 0.5999\n",
            "Epoch 423/1000 | Loss: 0.5997\n",
            "Epoch 424/1000 | Loss: 0.5994\n",
            "Epoch 425/1000 | Loss: 0.5992\n",
            "Epoch 426/1000 | Loss: 0.5990\n",
            "Epoch 427/1000 | Loss: 0.5987\n",
            "Epoch 428/1000 | Loss: 0.5985\n",
            "Epoch 429/1000 | Loss: 0.5982\n",
            "Epoch 430/1000 | Loss: 0.5980\n",
            "Epoch 431/1000 | Loss: 0.5978\n",
            "Epoch 432/1000 | Loss: 0.5975\n",
            "Epoch 433/1000 | Loss: 0.5973\n",
            "Epoch 434/1000 | Loss: 0.5971\n",
            "Epoch 435/1000 | Loss: 0.5968\n",
            "Epoch 436/1000 | Loss: 0.5966\n",
            "Epoch 437/1000 | Loss: 0.5964\n",
            "Epoch 438/1000 | Loss: 0.5961\n",
            "Epoch 439/1000 | Loss: 0.5959\n",
            "Epoch 440/1000 | Loss: 0.5957\n",
            "Epoch 441/1000 | Loss: 0.5954\n",
            "Epoch 442/1000 | Loss: 0.5952\n",
            "Epoch 443/1000 | Loss: 0.5950\n",
            "Epoch 444/1000 | Loss: 0.5947\n",
            "Epoch 445/1000 | Loss: 0.5945\n",
            "Epoch 446/1000 | Loss: 0.5943\n",
            "Epoch 447/1000 | Loss: 0.5940\n",
            "Epoch 448/1000 | Loss: 0.5938\n",
            "Epoch 449/1000 | Loss: 0.5936\n",
            "Epoch 450/1000 | Loss: 0.5933\n",
            "Epoch 451/1000 | Loss: 0.5931\n",
            "Epoch 452/1000 | Loss: 0.5929\n",
            "Epoch 453/1000 | Loss: 0.5926\n",
            "Epoch 454/1000 | Loss: 0.5924\n",
            "Epoch 455/1000 | Loss: 0.5922\n",
            "Epoch 456/1000 | Loss: 0.5919\n",
            "Epoch 457/1000 | Loss: 0.5917\n",
            "Epoch 458/1000 | Loss: 0.5915\n",
            "Epoch 459/1000 | Loss: 0.5913\n",
            "Epoch 460/1000 | Loss: 0.5910\n",
            "Epoch 461/1000 | Loss: 0.5908\n",
            "Epoch 462/1000 | Loss: 0.5906\n",
            "Epoch 463/1000 | Loss: 0.5903\n",
            "Epoch 464/1000 | Loss: 0.5901\n",
            "Epoch 465/1000 | Loss: 0.5899\n",
            "Epoch 466/1000 | Loss: 0.5896\n",
            "Epoch 467/1000 | Loss: 0.5894\n",
            "Epoch 468/1000 | Loss: 0.5892\n",
            "Epoch 469/1000 | Loss: 0.5890\n",
            "Epoch 470/1000 | Loss: 0.5887\n",
            "Epoch 471/1000 | Loss: 0.5885\n",
            "Epoch 472/1000 | Loss: 0.5883\n",
            "Epoch 473/1000 | Loss: 0.5880\n",
            "Epoch 474/1000 | Loss: 0.5878\n",
            "Epoch 475/1000 | Loss: 0.5876\n",
            "Epoch 476/1000 | Loss: 0.5874\n",
            "Epoch 477/1000 | Loss: 0.5871\n",
            "Epoch 478/1000 | Loss: 0.5869\n",
            "Epoch 479/1000 | Loss: 0.5867\n",
            "Epoch 480/1000 | Loss: 0.5864\n",
            "Epoch 481/1000 | Loss: 0.5862\n",
            "Epoch 482/1000 | Loss: 0.5860\n",
            "Epoch 483/1000 | Loss: 0.5858\n",
            "Epoch 484/1000 | Loss: 0.5855\n",
            "Epoch 485/1000 | Loss: 0.5853\n",
            "Epoch 486/1000 | Loss: 0.5851\n",
            "Epoch 487/1000 | Loss: 0.5849\n",
            "Epoch 488/1000 | Loss: 0.5846\n",
            "Epoch 489/1000 | Loss: 0.5844\n",
            "Epoch 490/1000 | Loss: 0.5842\n",
            "Epoch 491/1000 | Loss: 0.5840\n",
            "Epoch 492/1000 | Loss: 0.5837\n",
            "Epoch 493/1000 | Loss: 0.5835\n",
            "Epoch 494/1000 | Loss: 0.5833\n",
            "Epoch 495/1000 | Loss: 0.5831\n",
            "Epoch 496/1000 | Loss: 0.5828\n",
            "Epoch 497/1000 | Loss: 0.5826\n",
            "Epoch 498/1000 | Loss: 0.5824\n",
            "Epoch 499/1000 | Loss: 0.5822\n",
            "Epoch 500/1000 | Loss: 0.5819\n",
            "Epoch 501/1000 | Loss: 0.5817\n",
            "Epoch 502/1000 | Loss: 0.5815\n",
            "Epoch 503/1000 | Loss: 0.5813\n",
            "Epoch 504/1000 | Loss: 0.5810\n",
            "Epoch 505/1000 | Loss: 0.5808\n",
            "Epoch 506/1000 | Loss: 0.5806\n",
            "Epoch 507/1000 | Loss: 0.5804\n",
            "Epoch 508/1000 | Loss: 0.5801\n",
            "Epoch 509/1000 | Loss: 0.5799\n",
            "Epoch 510/1000 | Loss: 0.5797\n",
            "Epoch 511/1000 | Loss: 0.5795\n",
            "Epoch 512/1000 | Loss: 0.5793\n",
            "Epoch 513/1000 | Loss: 0.5790\n",
            "Epoch 514/1000 | Loss: 0.5788\n",
            "Epoch 515/1000 | Loss: 0.5786\n",
            "Epoch 516/1000 | Loss: 0.5784\n",
            "Epoch 517/1000 | Loss: 0.5781\n",
            "Epoch 518/1000 | Loss: 0.5779\n",
            "Epoch 519/1000 | Loss: 0.5777\n",
            "Epoch 520/1000 | Loss: 0.5775\n",
            "Epoch 521/1000 | Loss: 0.5773\n",
            "Epoch 522/1000 | Loss: 0.5770\n",
            "Epoch 523/1000 | Loss: 0.5768\n",
            "Epoch 524/1000 | Loss: 0.5766\n",
            "Epoch 525/1000 | Loss: 0.5764\n",
            "Epoch 526/1000 | Loss: 0.5762\n",
            "Epoch 527/1000 | Loss: 0.5759\n",
            "Epoch 528/1000 | Loss: 0.5757\n",
            "Epoch 529/1000 | Loss: 0.5755\n",
            "Epoch 530/1000 | Loss: 0.5753\n",
            "Epoch 531/1000 | Loss: 0.5751\n",
            "Epoch 532/1000 | Loss: 0.5748\n",
            "Epoch 533/1000 | Loss: 0.5746\n",
            "Epoch 534/1000 | Loss: 0.5744\n",
            "Epoch 535/1000 | Loss: 0.5742\n",
            "Epoch 536/1000 | Loss: 0.5740\n",
            "Epoch 537/1000 | Loss: 0.5737\n",
            "Epoch 538/1000 | Loss: 0.5735\n",
            "Epoch 539/1000 | Loss: 0.5733\n",
            "Epoch 540/1000 | Loss: 0.5731\n",
            "Epoch 541/1000 | Loss: 0.5729\n",
            "Epoch 542/1000 | Loss: 0.5727\n",
            "Epoch 543/1000 | Loss: 0.5724\n",
            "Epoch 544/1000 | Loss: 0.5722\n",
            "Epoch 545/1000 | Loss: 0.5720\n",
            "Epoch 546/1000 | Loss: 0.5718\n",
            "Epoch 547/1000 | Loss: 0.5716\n",
            "Epoch 548/1000 | Loss: 0.5714\n",
            "Epoch 549/1000 | Loss: 0.5711\n",
            "Epoch 550/1000 | Loss: 0.5709\n",
            "Epoch 551/1000 | Loss: 0.5707\n",
            "Epoch 552/1000 | Loss: 0.5705\n",
            "Epoch 553/1000 | Loss: 0.5703\n",
            "Epoch 554/1000 | Loss: 0.5701\n",
            "Epoch 555/1000 | Loss: 0.5698\n",
            "Epoch 556/1000 | Loss: 0.5696\n",
            "Epoch 557/1000 | Loss: 0.5694\n",
            "Epoch 558/1000 | Loss: 0.5692\n",
            "Epoch 559/1000 | Loss: 0.5690\n",
            "Epoch 560/1000 | Loss: 0.5688\n",
            "Epoch 561/1000 | Loss: 0.5685\n",
            "Epoch 562/1000 | Loss: 0.5683\n",
            "Epoch 563/1000 | Loss: 0.5681\n",
            "Epoch 564/1000 | Loss: 0.5679\n",
            "Epoch 565/1000 | Loss: 0.5677\n",
            "Epoch 566/1000 | Loss: 0.5675\n",
            "Epoch 567/1000 | Loss: 0.5673\n",
            "Epoch 568/1000 | Loss: 0.5670\n",
            "Epoch 569/1000 | Loss: 0.5668\n",
            "Epoch 570/1000 | Loss: 0.5666\n",
            "Epoch 571/1000 | Loss: 0.5664\n",
            "Epoch 572/1000 | Loss: 0.5662\n",
            "Epoch 573/1000 | Loss: 0.5660\n",
            "Epoch 574/1000 | Loss: 0.5658\n",
            "Epoch 575/1000 | Loss: 0.5656\n",
            "Epoch 576/1000 | Loss: 0.5653\n",
            "Epoch 577/1000 | Loss: 0.5651\n",
            "Epoch 578/1000 | Loss: 0.5649\n",
            "Epoch 579/1000 | Loss: 0.5647\n",
            "Epoch 580/1000 | Loss: 0.5645\n",
            "Epoch 581/1000 | Loss: 0.5643\n",
            "Epoch 582/1000 | Loss: 0.5641\n",
            "Epoch 583/1000 | Loss: 0.5639\n",
            "Epoch 584/1000 | Loss: 0.5636\n",
            "Epoch 585/1000 | Loss: 0.5634\n",
            "Epoch 586/1000 | Loss: 0.5632\n",
            "Epoch 587/1000 | Loss: 0.5630\n",
            "Epoch 588/1000 | Loss: 0.5628\n",
            "Epoch 589/1000 | Loss: 0.5626\n",
            "Epoch 590/1000 | Loss: 0.5624\n",
            "Epoch 591/1000 | Loss: 0.5622\n",
            "Epoch 592/1000 | Loss: 0.5620\n",
            "Epoch 593/1000 | Loss: 0.5617\n",
            "Epoch 594/1000 | Loss: 0.5615\n",
            "Epoch 595/1000 | Loss: 0.5613\n",
            "Epoch 596/1000 | Loss: 0.5611\n",
            "Epoch 597/1000 | Loss: 0.5609\n",
            "Epoch 598/1000 | Loss: 0.5607\n",
            "Epoch 599/1000 | Loss: 0.5605\n",
            "Epoch 600/1000 | Loss: 0.5603\n",
            "Epoch 601/1000 | Loss: 0.5601\n",
            "Epoch 602/1000 | Loss: 0.5599\n",
            "Epoch 603/1000 | Loss: 0.5597\n",
            "Epoch 604/1000 | Loss: 0.5594\n",
            "Epoch 605/1000 | Loss: 0.5592\n",
            "Epoch 606/1000 | Loss: 0.5590\n",
            "Epoch 607/1000 | Loss: 0.5588\n",
            "Epoch 608/1000 | Loss: 0.5586\n",
            "Epoch 609/1000 | Loss: 0.5584\n",
            "Epoch 610/1000 | Loss: 0.5582\n",
            "Epoch 611/1000 | Loss: 0.5580\n",
            "Epoch 612/1000 | Loss: 0.5578\n",
            "Epoch 613/1000 | Loss: 0.5576\n",
            "Epoch 614/1000 | Loss: 0.5574\n",
            "Epoch 615/1000 | Loss: 0.5572\n",
            "Epoch 616/1000 | Loss: 0.5570\n",
            "Epoch 617/1000 | Loss: 0.5568\n",
            "Epoch 618/1000 | Loss: 0.5565\n",
            "Epoch 619/1000 | Loss: 0.5563\n",
            "Epoch 620/1000 | Loss: 0.5561\n",
            "Epoch 621/1000 | Loss: 0.5559\n",
            "Epoch 622/1000 | Loss: 0.5557\n",
            "Epoch 623/1000 | Loss: 0.5555\n",
            "Epoch 624/1000 | Loss: 0.5553\n",
            "Epoch 625/1000 | Loss: 0.5551\n",
            "Epoch 626/1000 | Loss: 0.5549\n",
            "Epoch 627/1000 | Loss: 0.5547\n",
            "Epoch 628/1000 | Loss: 0.5545\n",
            "Epoch 629/1000 | Loss: 0.5543\n",
            "Epoch 630/1000 | Loss: 0.5541\n",
            "Epoch 631/1000 | Loss: 0.5539\n",
            "Epoch 632/1000 | Loss: 0.5537\n",
            "Epoch 633/1000 | Loss: 0.5535\n",
            "Epoch 634/1000 | Loss: 0.5533\n",
            "Epoch 635/1000 | Loss: 0.5531\n",
            "Epoch 636/1000 | Loss: 0.5529\n",
            "Epoch 637/1000 | Loss: 0.5526\n",
            "Epoch 638/1000 | Loss: 0.5524\n",
            "Epoch 639/1000 | Loss: 0.5522\n",
            "Epoch 640/1000 | Loss: 0.5520\n",
            "Epoch 641/1000 | Loss: 0.5518\n",
            "Epoch 642/1000 | Loss: 0.5516\n",
            "Epoch 643/1000 | Loss: 0.5514\n",
            "Epoch 644/1000 | Loss: 0.5512\n",
            "Epoch 645/1000 | Loss: 0.5510\n",
            "Epoch 646/1000 | Loss: 0.5508\n",
            "Epoch 647/1000 | Loss: 0.5506\n",
            "Epoch 648/1000 | Loss: 0.5504\n",
            "Epoch 649/1000 | Loss: 0.5502\n",
            "Epoch 650/1000 | Loss: 0.5500\n",
            "Epoch 651/1000 | Loss: 0.5498\n",
            "Epoch 652/1000 | Loss: 0.5496\n",
            "Epoch 653/1000 | Loss: 0.5494\n",
            "Epoch 654/1000 | Loss: 0.5492\n",
            "Epoch 655/1000 | Loss: 0.5490\n",
            "Epoch 656/1000 | Loss: 0.5488\n",
            "Epoch 657/1000 | Loss: 0.5486\n",
            "Epoch 658/1000 | Loss: 0.5484\n",
            "Epoch 659/1000 | Loss: 0.5482\n",
            "Epoch 660/1000 | Loss: 0.5480\n",
            "Epoch 661/1000 | Loss: 0.5478\n",
            "Epoch 662/1000 | Loss: 0.5476\n",
            "Epoch 663/1000 | Loss: 0.5474\n",
            "Epoch 664/1000 | Loss: 0.5472\n",
            "Epoch 665/1000 | Loss: 0.5470\n",
            "Epoch 666/1000 | Loss: 0.5468\n",
            "Epoch 667/1000 | Loss: 0.5466\n",
            "Epoch 668/1000 | Loss: 0.5464\n",
            "Epoch 669/1000 | Loss: 0.5462\n",
            "Epoch 670/1000 | Loss: 0.5460\n",
            "Epoch 671/1000 | Loss: 0.5458\n",
            "Epoch 672/1000 | Loss: 0.5456\n",
            "Epoch 673/1000 | Loss: 0.5454\n",
            "Epoch 674/1000 | Loss: 0.5452\n",
            "Epoch 675/1000 | Loss: 0.5450\n",
            "Epoch 676/1000 | Loss: 0.5448\n",
            "Epoch 677/1000 | Loss: 0.5446\n",
            "Epoch 678/1000 | Loss: 0.5444\n",
            "Epoch 679/1000 | Loss: 0.5442\n",
            "Epoch 680/1000 | Loss: 0.5440\n",
            "Epoch 681/1000 | Loss: 0.5438\n",
            "Epoch 682/1000 | Loss: 0.5436\n",
            "Epoch 683/1000 | Loss: 0.5434\n",
            "Epoch 684/1000 | Loss: 0.5432\n",
            "Epoch 685/1000 | Loss: 0.5430\n",
            "Epoch 686/1000 | Loss: 0.5428\n",
            "Epoch 687/1000 | Loss: 0.5426\n",
            "Epoch 688/1000 | Loss: 0.5424\n",
            "Epoch 689/1000 | Loss: 0.5422\n",
            "Epoch 690/1000 | Loss: 0.5420\n",
            "Epoch 691/1000 | Loss: 0.5419\n",
            "Epoch 692/1000 | Loss: 0.5417\n",
            "Epoch 693/1000 | Loss: 0.5415\n",
            "Epoch 694/1000 | Loss: 0.5413\n",
            "Epoch 695/1000 | Loss: 0.5411\n",
            "Epoch 696/1000 | Loss: 0.5409\n",
            "Epoch 697/1000 | Loss: 0.5407\n",
            "Epoch 698/1000 | Loss: 0.5405\n",
            "Epoch 699/1000 | Loss: 0.5403\n",
            "Epoch 700/1000 | Loss: 0.5401\n",
            "Epoch 701/1000 | Loss: 0.5399\n",
            "Epoch 702/1000 | Loss: 0.5397\n",
            "Epoch 703/1000 | Loss: 0.5395\n",
            "Epoch 704/1000 | Loss: 0.5393\n",
            "Epoch 705/1000 | Loss: 0.5391\n",
            "Epoch 706/1000 | Loss: 0.5389\n",
            "Epoch 707/1000 | Loss: 0.5387\n",
            "Epoch 708/1000 | Loss: 0.5385\n",
            "Epoch 709/1000 | Loss: 0.5383\n",
            "Epoch 710/1000 | Loss: 0.5381\n",
            "Epoch 711/1000 | Loss: 0.5380\n",
            "Epoch 712/1000 | Loss: 0.5378\n",
            "Epoch 713/1000 | Loss: 0.5376\n",
            "Epoch 714/1000 | Loss: 0.5374\n",
            "Epoch 715/1000 | Loss: 0.5372\n",
            "Epoch 716/1000 | Loss: 0.5370\n",
            "Epoch 717/1000 | Loss: 0.5368\n",
            "Epoch 718/1000 | Loss: 0.5366\n",
            "Epoch 719/1000 | Loss: 0.5364\n",
            "Epoch 720/1000 | Loss: 0.5362\n",
            "Epoch 721/1000 | Loss: 0.5360\n",
            "Epoch 722/1000 | Loss: 0.5358\n",
            "Epoch 723/1000 | Loss: 0.5356\n",
            "Epoch 724/1000 | Loss: 0.5354\n",
            "Epoch 725/1000 | Loss: 0.5353\n",
            "Epoch 726/1000 | Loss: 0.5351\n",
            "Epoch 727/1000 | Loss: 0.5349\n",
            "Epoch 728/1000 | Loss: 0.5347\n",
            "Epoch 729/1000 | Loss: 0.5345\n",
            "Epoch 730/1000 | Loss: 0.5343\n",
            "Epoch 731/1000 | Loss: 0.5341\n",
            "Epoch 732/1000 | Loss: 0.5339\n",
            "Epoch 733/1000 | Loss: 0.5337\n",
            "Epoch 734/1000 | Loss: 0.5335\n",
            "Epoch 735/1000 | Loss: 0.5333\n",
            "Epoch 736/1000 | Loss: 0.5332\n",
            "Epoch 737/1000 | Loss: 0.5330\n",
            "Epoch 738/1000 | Loss: 0.5328\n",
            "Epoch 739/1000 | Loss: 0.5326\n",
            "Epoch 740/1000 | Loss: 0.5324\n",
            "Epoch 741/1000 | Loss: 0.5322\n",
            "Epoch 742/1000 | Loss: 0.5320\n",
            "Epoch 743/1000 | Loss: 0.5318\n",
            "Epoch 744/1000 | Loss: 0.5316\n",
            "Epoch 745/1000 | Loss: 0.5315\n",
            "Epoch 746/1000 | Loss: 0.5313\n",
            "Epoch 747/1000 | Loss: 0.5311\n",
            "Epoch 748/1000 | Loss: 0.5309\n",
            "Epoch 749/1000 | Loss: 0.5307\n",
            "Epoch 750/1000 | Loss: 0.5305\n",
            "Epoch 751/1000 | Loss: 0.5303\n",
            "Epoch 752/1000 | Loss: 0.5301\n",
            "Epoch 753/1000 | Loss: 0.5299\n",
            "Epoch 754/1000 | Loss: 0.5298\n",
            "Epoch 755/1000 | Loss: 0.5296\n",
            "Epoch 756/1000 | Loss: 0.5294\n",
            "Epoch 757/1000 | Loss: 0.5292\n",
            "Epoch 758/1000 | Loss: 0.5290\n",
            "Epoch 759/1000 | Loss: 0.5288\n",
            "Epoch 760/1000 | Loss: 0.5286\n",
            "Epoch 761/1000 | Loss: 0.5284\n",
            "Epoch 762/1000 | Loss: 0.5283\n",
            "Epoch 763/1000 | Loss: 0.5281\n",
            "Epoch 764/1000 | Loss: 0.5279\n",
            "Epoch 765/1000 | Loss: 0.5277\n",
            "Epoch 766/1000 | Loss: 0.5275\n",
            "Epoch 767/1000 | Loss: 0.5273\n",
            "Epoch 768/1000 | Loss: 0.5271\n",
            "Epoch 769/1000 | Loss: 0.5270\n",
            "Epoch 770/1000 | Loss: 0.5268\n",
            "Epoch 771/1000 | Loss: 0.5266\n",
            "Epoch 772/1000 | Loss: 0.5264\n",
            "Epoch 773/1000 | Loss: 0.5262\n",
            "Epoch 774/1000 | Loss: 0.5260\n",
            "Epoch 775/1000 | Loss: 0.5258\n",
            "Epoch 776/1000 | Loss: 0.5257\n",
            "Epoch 777/1000 | Loss: 0.5255\n",
            "Epoch 778/1000 | Loss: 0.5253\n",
            "Epoch 779/1000 | Loss: 0.5251\n",
            "Epoch 780/1000 | Loss: 0.5249\n",
            "Epoch 781/1000 | Loss: 0.5247\n",
            "Epoch 782/1000 | Loss: 0.5245\n",
            "Epoch 783/1000 | Loss: 0.5244\n",
            "Epoch 784/1000 | Loss: 0.5242\n",
            "Epoch 785/1000 | Loss: 0.5240\n",
            "Epoch 786/1000 | Loss: 0.5238\n",
            "Epoch 787/1000 | Loss: 0.5236\n",
            "Epoch 788/1000 | Loss: 0.5234\n",
            "Epoch 789/1000 | Loss: 0.5233\n",
            "Epoch 790/1000 | Loss: 0.5231\n",
            "Epoch 791/1000 | Loss: 0.5229\n",
            "Epoch 792/1000 | Loss: 0.5227\n",
            "Epoch 793/1000 | Loss: 0.5225\n",
            "Epoch 794/1000 | Loss: 0.5223\n",
            "Epoch 795/1000 | Loss: 0.5222\n",
            "Epoch 796/1000 | Loss: 0.5220\n",
            "Epoch 797/1000 | Loss: 0.5218\n",
            "Epoch 798/1000 | Loss: 0.5216\n",
            "Epoch 799/1000 | Loss: 0.5214\n",
            "Epoch 800/1000 | Loss: 0.5212\n",
            "Epoch 801/1000 | Loss: 0.5211\n",
            "Epoch 802/1000 | Loss: 0.5209\n",
            "Epoch 803/1000 | Loss: 0.5207\n",
            "Epoch 804/1000 | Loss: 0.5205\n",
            "Epoch 805/1000 | Loss: 0.5203\n",
            "Epoch 806/1000 | Loss: 0.5202\n",
            "Epoch 807/1000 | Loss: 0.5200\n",
            "Epoch 808/1000 | Loss: 0.5198\n",
            "Epoch 809/1000 | Loss: 0.5196\n",
            "Epoch 810/1000 | Loss: 0.5194\n",
            "Epoch 811/1000 | Loss: 0.5193\n",
            "Epoch 812/1000 | Loss: 0.5191\n",
            "Epoch 813/1000 | Loss: 0.5189\n",
            "Epoch 814/1000 | Loss: 0.5187\n",
            "Epoch 815/1000 | Loss: 0.5185\n",
            "Epoch 816/1000 | Loss: 0.5184\n",
            "Epoch 817/1000 | Loss: 0.5182\n",
            "Epoch 818/1000 | Loss: 0.5180\n",
            "Epoch 819/1000 | Loss: 0.5178\n",
            "Epoch 820/1000 | Loss: 0.5176\n",
            "Epoch 821/1000 | Loss: 0.5175\n",
            "Epoch 822/1000 | Loss: 0.5173\n",
            "Epoch 823/1000 | Loss: 0.5171\n",
            "Epoch 824/1000 | Loss: 0.5169\n",
            "Epoch 825/1000 | Loss: 0.5167\n",
            "Epoch 826/1000 | Loss: 0.5166\n",
            "Epoch 827/1000 | Loss: 0.5164\n",
            "Epoch 828/1000 | Loss: 0.5162\n",
            "Epoch 829/1000 | Loss: 0.5160\n",
            "Epoch 830/1000 | Loss: 0.5158\n",
            "Epoch 831/1000 | Loss: 0.5157\n",
            "Epoch 832/1000 | Loss: 0.5155\n",
            "Epoch 833/1000 | Loss: 0.5153\n",
            "Epoch 834/1000 | Loss: 0.5151\n",
            "Epoch 835/1000 | Loss: 0.5150\n",
            "Epoch 836/1000 | Loss: 0.5148\n",
            "Epoch 837/1000 | Loss: 0.5146\n",
            "Epoch 838/1000 | Loss: 0.5144\n",
            "Epoch 839/1000 | Loss: 0.5142\n",
            "Epoch 840/1000 | Loss: 0.5141\n",
            "Epoch 841/1000 | Loss: 0.5139\n",
            "Epoch 842/1000 | Loss: 0.5137\n",
            "Epoch 843/1000 | Loss: 0.5135\n",
            "Epoch 844/1000 | Loss: 0.5134\n",
            "Epoch 845/1000 | Loss: 0.5132\n",
            "Epoch 846/1000 | Loss: 0.5130\n",
            "Epoch 847/1000 | Loss: 0.5128\n",
            "Epoch 848/1000 | Loss: 0.5127\n",
            "Epoch 849/1000 | Loss: 0.5125\n",
            "Epoch 850/1000 | Loss: 0.5123\n",
            "Epoch 851/1000 | Loss: 0.5121\n",
            "Epoch 852/1000 | Loss: 0.5119\n",
            "Epoch 853/1000 | Loss: 0.5118\n",
            "Epoch 854/1000 | Loss: 0.5116\n",
            "Epoch 855/1000 | Loss: 0.5114\n",
            "Epoch 856/1000 | Loss: 0.5112\n",
            "Epoch 857/1000 | Loss: 0.5111\n",
            "Epoch 858/1000 | Loss: 0.5109\n",
            "Epoch 859/1000 | Loss: 0.5107\n",
            "Epoch 860/1000 | Loss: 0.5105\n",
            "Epoch 861/1000 | Loss: 0.5104\n",
            "Epoch 862/1000 | Loss: 0.5102\n",
            "Epoch 863/1000 | Loss: 0.5100\n",
            "Epoch 864/1000 | Loss: 0.5099\n",
            "Epoch 865/1000 | Loss: 0.5097\n",
            "Epoch 866/1000 | Loss: 0.5095\n",
            "Epoch 867/1000 | Loss: 0.5093\n",
            "Epoch 868/1000 | Loss: 0.5092\n",
            "Epoch 869/1000 | Loss: 0.5090\n",
            "Epoch 870/1000 | Loss: 0.5088\n",
            "Epoch 871/1000 | Loss: 0.5086\n",
            "Epoch 872/1000 | Loss: 0.5085\n",
            "Epoch 873/1000 | Loss: 0.5083\n",
            "Epoch 874/1000 | Loss: 0.5081\n",
            "Epoch 875/1000 | Loss: 0.5079\n",
            "Epoch 876/1000 | Loss: 0.5078\n",
            "Epoch 877/1000 | Loss: 0.5076\n",
            "Epoch 878/1000 | Loss: 0.5074\n",
            "Epoch 879/1000 | Loss: 0.5072\n",
            "Epoch 880/1000 | Loss: 0.5071\n",
            "Epoch 881/1000 | Loss: 0.5069\n",
            "Epoch 882/1000 | Loss: 0.5067\n",
            "Epoch 883/1000 | Loss: 0.5066\n",
            "Epoch 884/1000 | Loss: 0.5064\n",
            "Epoch 885/1000 | Loss: 0.5062\n",
            "Epoch 886/1000 | Loss: 0.5060\n",
            "Epoch 887/1000 | Loss: 0.5059\n",
            "Epoch 888/1000 | Loss: 0.5057\n",
            "Epoch 889/1000 | Loss: 0.5055\n",
            "Epoch 890/1000 | Loss: 0.5054\n",
            "Epoch 891/1000 | Loss: 0.5052\n",
            "Epoch 892/1000 | Loss: 0.5050\n",
            "Epoch 893/1000 | Loss: 0.5048\n",
            "Epoch 894/1000 | Loss: 0.5047\n",
            "Epoch 895/1000 | Loss: 0.5045\n",
            "Epoch 896/1000 | Loss: 0.5043\n",
            "Epoch 897/1000 | Loss: 0.5042\n",
            "Epoch 898/1000 | Loss: 0.5040\n",
            "Epoch 899/1000 | Loss: 0.5038\n",
            "Epoch 900/1000 | Loss: 0.5037\n",
            "Epoch 901/1000 | Loss: 0.5035\n",
            "Epoch 902/1000 | Loss: 0.5033\n",
            "Epoch 903/1000 | Loss: 0.5031\n",
            "Epoch 904/1000 | Loss: 0.5030\n",
            "Epoch 905/1000 | Loss: 0.5028\n",
            "Epoch 906/1000 | Loss: 0.5026\n",
            "Epoch 907/1000 | Loss: 0.5025\n",
            "Epoch 908/1000 | Loss: 0.5023\n",
            "Epoch 909/1000 | Loss: 0.5021\n",
            "Epoch 910/1000 | Loss: 0.5020\n",
            "Epoch 911/1000 | Loss: 0.5018\n",
            "Epoch 912/1000 | Loss: 0.5016\n",
            "Epoch 913/1000 | Loss: 0.5015\n",
            "Epoch 914/1000 | Loss: 0.5013\n",
            "Epoch 915/1000 | Loss: 0.5011\n",
            "Epoch 916/1000 | Loss: 0.5009\n",
            "Epoch 917/1000 | Loss: 0.5008\n",
            "Epoch 918/1000 | Loss: 0.5006\n",
            "Epoch 919/1000 | Loss: 0.5004\n",
            "Epoch 920/1000 | Loss: 0.5003\n",
            "Epoch 921/1000 | Loss: 0.5001\n",
            "Epoch 922/1000 | Loss: 0.4999\n",
            "Epoch 923/1000 | Loss: 0.4998\n",
            "Epoch 924/1000 | Loss: 0.4996\n",
            "Epoch 925/1000 | Loss: 0.4994\n",
            "Epoch 926/1000 | Loss: 0.4993\n",
            "Epoch 927/1000 | Loss: 0.4991\n",
            "Epoch 928/1000 | Loss: 0.4989\n",
            "Epoch 929/1000 | Loss: 0.4988\n",
            "Epoch 930/1000 | Loss: 0.4986\n",
            "Epoch 931/1000 | Loss: 0.4984\n",
            "Epoch 932/1000 | Loss: 0.4983\n",
            "Epoch 933/1000 | Loss: 0.4981\n",
            "Epoch 934/1000 | Loss: 0.4979\n",
            "Epoch 935/1000 | Loss: 0.4978\n",
            "Epoch 936/1000 | Loss: 0.4976\n",
            "Epoch 937/1000 | Loss: 0.4974\n",
            "Epoch 938/1000 | Loss: 0.4973\n",
            "Epoch 939/1000 | Loss: 0.4971\n",
            "Epoch 940/1000 | Loss: 0.4969\n",
            "Epoch 941/1000 | Loss: 0.4968\n",
            "Epoch 942/1000 | Loss: 0.4966\n",
            "Epoch 943/1000 | Loss: 0.4964\n",
            "Epoch 944/1000 | Loss: 0.4963\n",
            "Epoch 945/1000 | Loss: 0.4961\n",
            "Epoch 946/1000 | Loss: 0.4960\n",
            "Epoch 947/1000 | Loss: 0.4958\n",
            "Epoch 948/1000 | Loss: 0.4956\n",
            "Epoch 949/1000 | Loss: 0.4955\n",
            "Epoch 950/1000 | Loss: 0.4953\n",
            "Epoch 951/1000 | Loss: 0.4951\n",
            "Epoch 952/1000 | Loss: 0.4950\n",
            "Epoch 953/1000 | Loss: 0.4948\n",
            "Epoch 954/1000 | Loss: 0.4946\n",
            "Epoch 955/1000 | Loss: 0.4945\n",
            "Epoch 956/1000 | Loss: 0.4943\n",
            "Epoch 957/1000 | Loss: 0.4941\n",
            "Epoch 958/1000 | Loss: 0.4940\n",
            "Epoch 959/1000 | Loss: 0.4938\n",
            "Epoch 960/1000 | Loss: 0.4937\n",
            "Epoch 961/1000 | Loss: 0.4935\n",
            "Epoch 962/1000 | Loss: 0.4933\n",
            "Epoch 963/1000 | Loss: 0.4932\n",
            "Epoch 964/1000 | Loss: 0.4930\n",
            "Epoch 965/1000 | Loss: 0.4928\n",
            "Epoch 966/1000 | Loss: 0.4927\n",
            "Epoch 967/1000 | Loss: 0.4925\n",
            "Epoch 968/1000 | Loss: 0.4924\n",
            "Epoch 969/1000 | Loss: 0.4922\n",
            "Epoch 970/1000 | Loss: 0.4920\n",
            "Epoch 971/1000 | Loss: 0.4919\n",
            "Epoch 972/1000 | Loss: 0.4917\n",
            "Epoch 973/1000 | Loss: 0.4915\n",
            "Epoch 974/1000 | Loss: 0.4914\n",
            "Epoch 975/1000 | Loss: 0.4912\n",
            "Epoch 976/1000 | Loss: 0.4911\n",
            "Epoch 977/1000 | Loss: 0.4909\n",
            "Epoch 978/1000 | Loss: 0.4907\n",
            "Epoch 979/1000 | Loss: 0.4906\n",
            "Epoch 980/1000 | Loss: 0.4904\n",
            "Epoch 981/1000 | Loss: 0.4903\n",
            "Epoch 982/1000 | Loss: 0.4901\n",
            "Epoch 983/1000 | Loss: 0.4899\n",
            "Epoch 984/1000 | Loss: 0.4898\n",
            "Epoch 985/1000 | Loss: 0.4896\n",
            "Epoch 986/1000 | Loss: 0.4894\n",
            "Epoch 987/1000 | Loss: 0.4893\n",
            "Epoch 988/1000 | Loss: 0.4891\n",
            "Epoch 989/1000 | Loss: 0.4890\n",
            "Epoch 990/1000 | Loss: 0.4888\n",
            "Epoch 991/1000 | Loss: 0.4886\n",
            "Epoch 992/1000 | Loss: 0.4885\n",
            "Epoch 993/1000 | Loss: 0.4883\n",
            "Epoch 994/1000 | Loss: 0.4882\n",
            "Epoch 995/1000 | Loss: 0.4880\n",
            "Epoch 996/1000 | Loss: 0.4879\n",
            "Epoch 997/1000 | Loss: 0.4877\n",
            "Epoch 998/1000 | Loss: 0.4875\n",
            "Epoch 999/1000 | Loss: 0.4874\n",
            "Epoch 1000/1000 | Loss: 0.4872\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.4174 | Above 50%: False\n",
            "Prediction after 7 hours of training: 0.9581 | Above 50%: True\n"
          ]
        }
      ]
    }
  ]
}